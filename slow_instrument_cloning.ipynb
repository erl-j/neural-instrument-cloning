{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3kBReFZb1eKk"
   },
   "outputs": [],
   "source": [
    "# imports and utils\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "import ddsp.training\n",
    "_AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "from IPython.display import Audio, display\n",
    "from livelossplot import PlotLosses\n",
    "import numpy as np\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import time\n",
    "import data\n",
    "\n",
    "# define constants\n",
    "N_SAMPLES=16000*4\n",
    "SAMPLE_RATE=16000\n",
    "SEED=1\n",
    "\n",
    "tf.random.set_seed(\n",
    "    SEED\n",
    ")\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "\n",
    "\n",
    "# define some utilis\n",
    "def play(audio):\n",
    "  display(Audio(audio,rate=SAMPLE_RATE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "USE_NSYNTH=False\n",
    "if USE_NSYNTH:\n",
    "    \n",
    "    class CustomNSynthTfds(ddsp.training.data.TfdsProvider):\n",
    "      \"\"\"Parses features in the TFDS NSynth dataset.\n",
    "      Unlike the default Nsynth data provider, this class keeps the the nsynth instrument metadata.\n",
    "\n",
    "      If running on Cloud, it is recommended you set `data_dir` to\n",
    "      'gs://tfds-data/datasets' to avoid unnecessary downloads.\n",
    "      \"\"\"\n",
    "\n",
    "      def __init__(self,\n",
    "                   name='nsynth/gansynth_subset.f0_and_loudness:2.3.3',\n",
    "                   split='train',\n",
    "                   data_dir='gs://tfds-data/datasets',\n",
    "                   sample_rate=16000,\n",
    "                   frame_rate=250,\n",
    "                   include_note_labels=True):\n",
    "        \"\"\"TfdsProvider constructor.\n",
    "        Args:\n",
    "          name: TFDS dataset name (with optional config and version).\n",
    "          split: Dataset split to use of the TFDS dataset.\n",
    "          data_dir: The directory to read the prepared NSynth dataset from. Defaults\n",
    "            to the public TFDS GCS bucket.\n",
    "          sample_rate: Sample rate of audio in the dataset.\n",
    "          frame_rate: Frame rate of features in the dataset.\n",
    "          include_note_labels: Return dataset without note-level labels\n",
    "            (pitch, instrument).\n",
    "        \"\"\"\n",
    "        self._include_note_labels = include_note_labels\n",
    "\n",
    "        super().__init__(name, split, data_dir, sample_rate, frame_rate)\n",
    "\n",
    "      def get_dataset(self, shuffle=True):\n",
    "        \"\"\"Returns dataset with slight restructuring of feature dictionary.\"\"\"\n",
    "        def preprocess_ex(ex):\n",
    "          ex_out = {\n",
    "              'audio':\n",
    "                  ex['audio'],\n",
    "              'f0_hz':\n",
    "                  ex['f0']['hz'],\n",
    "              'f0_confidence':\n",
    "                  ex['f0']['confidence'],\n",
    "              'loudness_db':\n",
    "                  ex['loudness']['db'],\n",
    "          }\n",
    "          if self._include_note_labels:\n",
    "            ex_out.update({\n",
    "                'pitch':\n",
    "                    ex['pitch'],\n",
    "                'velocity':\n",
    "                    ex['velocity'],\n",
    "                'instrument_source':\n",
    "                    ex['instrument']['source'],\n",
    "                'instrument_family':\n",
    "                    ex['instrument']['family'],\n",
    "                'instrument':\n",
    "                    ex['instrument']['label'],\n",
    "            })\n",
    "          return ex_out\n",
    "\n",
    "        dataset = super().get_dataset(shuffle)\n",
    "        dataset = dataset.map(preprocess_ex, num_parallel_calls=_AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    tfds.load(\"nsynth/gansynth_subset.f0_and_loudness\",split=\"train\", try_gcs=False,download=True) \n",
    "    trn_data_provider = CustomNSynthTfds(data_dir=\"/root/tensorflow_datasets/\",split=\"train\")\n",
    "\n",
    "    tfds.load(\"nsynth/gansynth_subset.f0_and_loudness\",split=\"valid\", try_gcs=False,download=True) \n",
    "    val_data_provider = CustomNSynthTfds(data_dir=\"/root/tensorflow_datasets/\",split=\"valid\")\n",
    "    \n",
    "    # take a batch of flute sounds\n",
    "    #dataset = data_provider.get_dataset()\n",
    "\n",
    "    # take only acoustic sounds\n",
    "    #dataset=dataset.filter(lambda x: x[\"instrument_source\"]==0)\n",
    "\n",
    "    # take only flute sounds\n",
    "    #dataset=dataset.filter(lambda x: x[\"instrument_family\"]==2)\n",
    "\n",
    "    # flutes\n",
    "    # 2965 samples\n",
    "    # 36 instruments\n",
    "    # 5 velocities \n",
    "    # 61 pitches\n",
    "\n",
    "    #test_batch=next(iter(dataset.batch(4))) \n",
    "\n",
    "    #play(tf.reshape(test_batch[\"audio\"],(-1)))\n",
    "\n",
    "    # TODO: CLEAN UP DATASET. REMOVE WHERE CREPE IS UNCERTAIN.\n",
    "\n",
    "    def crepe_is_certain(x):\n",
    "        is_playing = tf.cast(x[\"loudness_db\"]>-100.0,dtype=tf.float32)\n",
    "        average_certainty=tf.reduce_sum(x[\"f0_confidence\"]*is_playing)/tf.reduce_sum(is_playing)\n",
    "        return average_certainty\n",
    "\n",
    "    '''\n",
    "    for s in range(1):\n",
    "\n",
    "        print(\"###\")\n",
    "\n",
    "        sample = next(iter(dataset))\n",
    "        play(sample[\"audio\"])\n",
    "\n",
    "        print(crepe_is_certain(sample))\n",
    "\n",
    "        plt.plot(sample[\"loudness_db\"])\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(sample[\"f0_hz\"])\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(sample[\"f0_confidence\"])\n",
    "        plt.show()\n",
    "\n",
    "    '''\n",
    "\n",
    "    INSTRUMENT_FAMILY=2\n",
    "\n",
    "    def preprocess_dataset(dataset):\n",
    "        if INSTRUMENT_FAMILY!=\"all\":\n",
    "            dataset=dataset.filter(lambda x: x[\"instrument_family\"]==INSTRUMENT_FAMILY)\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    trn_dataset = preprocess_dataset(trn_data_provider.get_dataset())\n",
    "    val_dataset = preprocess_dataset(val_data_provider.get_dataset())\n",
    "\n",
    "    # take only flute sounds\n",
    "\n",
    "\n",
    "else:\n",
    "    INSTRUMENT_FAMILY=\"solos-violin-clean\"\n",
    "    \n",
    "    trn_data_provider=data.MultiTFRecordProvider(f\"datasets/{INSTRUMENT_FAMILY}/tfr/trn/*\")\n",
    "    val_data_provider=data.MultiTFRecordProvider(f\"datasets/{INSTRUMENT_FAMILY}/tfr/val/*\")\n",
    "    \n",
    "    trn_dataset= trn_data_provider.get_dataset()\n",
    "    val_dataset=val_data_provider.get_dataset()\n",
    "    \n",
    "    def preprocess_dataset(dataset):\n",
    "        dataset=dataset.filter(lambda x: not tf.math.is_nan(x[\"audio\"]).any())\n",
    "        return dataset\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Kj83Q3YrDEeT"
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "N_NOISE_MAGNITUDES=64\n",
    "N_HARMONICS=64\n",
    "\n",
    "\n",
    "class CustomReverb(ddsp.processors.Processor):\n",
    "\n",
    "    def __init__(self,name='reverb'):\n",
    "        \"\"\"Takes neural network outputs directly as the impulse response.\n",
    "        Args:\n",
    "          trainable: Learn the impulse_response as a single variable for the entire\n",
    "            dataset.\n",
    "          reverb_length: Length of the impulse response. Only used if\n",
    "            trainable=True.\n",
    "          add_dry: Add dry signal to reverberated signal on output.\n",
    "          name: Name of processor module.\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "    \n",
    "    def get_controls(self, audio, ir=None):\n",
    "        \"\"\"Convert decoder outputs into ir response.\n",
    "        Args:\n",
    "          audio: Dry audio. 2-D Tensor of shape [batch, n_samples].\n",
    "          ir: 3-D Tensor of shape [batch, ir_size, 1] or 2D Tensor of shape\n",
    "            [batch, ir_size].\n",
    "        Returns:\n",
    "          controls: Dictionary of effect controls.\n",
    "        \"\"\"\n",
    "        return {'audio': audio, 'ir': ir}\n",
    "\n",
    "    def get_signal(self, audio, ir):\n",
    "        \"\"\"Apply impulse response.\n",
    "        Args:\n",
    "          audio: Dry audio, 2-D Tensor of shape [batch, n_samples].\n",
    "          ir: 3-D Tensor of shape [batch, ir_size, 1] or 2D Tensor of shape\n",
    "            [batch, ir_size].\n",
    "        Returns:\n",
    "          tensor of shape [batch, n_samples]\n",
    "        \"\"\"\n",
    "        wet = ddsp.core.fft_convolve(audio, ir, padding='same', delay_compensation=0)\n",
    "        return wet\n",
    "\n",
    "class LGD(ddsp.training.models.Model):\n",
    "\n",
    "    def __init__(self, n_out,  rnn_channels=256, rnn_type='gru', ch=512, layers_per_stack=0, name=None):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        def stack():\n",
    "            return ddsp.training.nn.FcStack(ch, layers_per_stack)\n",
    "\n",
    "        self.input_keys = [\"f0_hz\", \"ld_scaled\"]\n",
    "\n",
    "        self.n_out = n_out\n",
    "        self.input_stacks = [stack() for k in self.input_keys]\n",
    "\n",
    "        self.rnn = ddsp.training.nn.Rnn(rnn_channels, rnn_type)\n",
    "        self.out_stack = stack()\n",
    "        self.dense_out = ddsp.training.nn.Fc(self.n_out)\n",
    "\n",
    "    def call(self, conditioning):\n",
    "\n",
    "        instrument_representation = conditioning[\"z\"]\n",
    "\n",
    "        feature_len = conditioning[\"f0_scaled\"].shape[1]\n",
    "\n",
    "        instrument_representation = tf.repeat(\n",
    "            instrument_representation[:, None, ...], feature_len, axis=1)\n",
    "\n",
    "        inputs = [conditioning[k] for k in self.input_keys]\n",
    "\n",
    "        inputs = [stack(x) for stack, x in zip(self.input_stacks, inputs)]\n",
    "\n",
    "        x = tf.concat(inputs, axis=(-1))\n",
    "        x = tf.concat([x, instrument_representation], axis=2)\n",
    "        x = self.rnn(x)\n",
    "        x = tf.concat((inputs + [x]), axis=(-1))\n",
    "        x = self.out_stack(x)\n",
    "        out = self.dense_out(x)\n",
    "        \n",
    "        \n",
    "        loudness=out[:,:,-1:]\n",
    "        noise_magnitudes = out[:,:,:N_NOISE_MAGNITUDES]\n",
    "        harmonic_distribution = out[:,:,-N_HARMONICS:-1]\n",
    "            \n",
    "        return {\"amps\":loudness,\"harmonic_distribution\":harmonic_distribution,\"magnitudes\":noise_magnitudes}\n",
    "    \n",
    "preprocessor=ddsp.training.preprocessing.F0LoudnessPreprocessor()\n",
    "decoder = LGD(n_out=N_NOISE_MAGNITUDES+N_HARMONICS+1)\n",
    "harmonic_synth = ddsp.synths.Harmonic(\n",
    "    n_samples=N_SAMPLES, sample_rate=SAMPLE_RATE, name='harmonic')\n",
    "\n",
    "filtered_noise = ddsp.synths.FilteredNoise(\n",
    "    n_samples=N_SAMPLES, window_size=0, initial_bias=-10.0, name='noise')\n",
    "reverb = ddsp.effects.Reverb(name=\"reverb\",trainable=False)\n",
    "add = ddsp.processors.Add(name='add')\n",
    "\n",
    "dag = [\n",
    "  (harmonic_synth, ['amps', 'harmonic_distribution', 'f0_hz']),\n",
    "  (filtered_noise, ['magnitudes']),\n",
    "  (add, ['harmonic/signal', 'noise/signal']),\n",
    "  (reverb, [\"add/signal\",\"ir\"])\n",
    "]\n",
    "\n",
    "processor_group=ddsp.processors.ProcessorGroup(dag=dag)\n",
    "\n",
    "spectral_loss = ddsp.losses.SpectralLoss(loss_type='L1',\n",
    "                                              mag_weight=1.0,\n",
    "                                              logmag_weight=1.0)\n",
    "\n",
    "\n",
    "class MultiInstrumentAutoencoder(ddsp.training.models.autoencoder.Autoencoder):\n",
    "    def __init__(self,\n",
    "               preprocessor=None,\n",
    "               encoder=None,\n",
    "               decoder=None,\n",
    "               processor_group=None,\n",
    "               losses=None,\n",
    "               n_instruments=None,\n",
    "               z_size=None,\n",
    "               ir_size=None,\n",
    "               **kwargs):\n",
    "        super().__init__(preprocessor,encoder,decoder,processor_group,losses,**kwargs)\n",
    "        \n",
    "        self.n_instruments=n_instruments\n",
    "        self.instrument_z = tf.Variable(tf.random.normal([n_instruments,z_size]))\n",
    "        self.instrument_ir = tf.Variable(tf.concat([tf.ones([n_instruments,1]),tf.zeros([n_instruments,ir_size-1])],axis=-1))\n",
    "        self.instrument_id2idx={}\n",
    "        \n",
    "    def call(self, batch, training=True):\n",
    "        instrument_idxs=[]\n",
    " \n",
    "        for sample_index in range(batch[\"instrument\"].shape[0]):\n",
    "          instrument_id=str(batch[\"instrument\"][sample_index])\n",
    "          if instrument_id not in self.instrument_id2idx:\n",
    "              self.instrument_id2idx[instrument_id]=len(self.instrument_id2idx.keys())\n",
    "          instrument_idxs.append(int(self.instrument_id2idx[instrument_id]))\n",
    "  \n",
    "        with tf.GradientTape() as tape:\n",
    "          batch[\"z\"]=tf.tanh(tf.gather(self.instrument_z,instrument_idxs))\n",
    "          batch[\"ir\"]=tf.tanh(tf.gather(self.instrument_ir,instrument_idxs))     \n",
    "        return super().call(batch,training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wJUeq9Kmwbga",
    "outputId": "d27f1547-a061-4bf7-d3d6-f5715199fb3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7')\n"
     ]
    }
   ],
   "source": [
    "Z_SIZE=256\n",
    "\n",
    "N_INSTRUMENTS=1050\n",
    "\n",
    "IR_SIZE=int(SAMPLE_RATE*1.0)\n",
    "\n",
    "plotlosses = PlotLosses()\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
    "\n",
    "def train_step(inputs):\n",
    "     \n",
    "    with tf.GradientTape() as tape:\n",
    "    \n",
    "        a = time.time()\n",
    "\n",
    "        output=ae(inputs,training=False)\n",
    "\n",
    "        loss_value=spectral_loss(inputs[\"audio\"],output['audio_synth'])\n",
    "\n",
    "        gradients = tape.gradient(loss_value, [*ae.trainable_variables])\n",
    "\n",
    "        #grad_is_nan=False\n",
    "        #for g in gradients:\n",
    "        #    grad_is_nan=grad_is_nan or tf.math.reduce_any(tf.math.is_nan(g))\n",
    "        \n",
    "\n",
    "        # if not grad_is_nan:\n",
    "        optimizer.apply_gradients(zip(gradients, [*ae.trainable_variables]))\n",
    "        \n",
    "    return loss_value\n",
    "    \n",
    "\n",
    "@tf.function\n",
    "def distributed_train_step(dataset_inputs):\n",
    "    per_replica_losses=strategy.run(train_step,args=(dataset_inputs,))\n",
    "    return strategy.reduce(tf.distribute.ReduceOp.SUM,per_replica_losses,axis=None)\n",
    "\n",
    "with strategy.scope():\n",
    "    ae = MultiInstrumentAutoencoder(preprocessor=preprocessor,decoder=decoder,processor_group=processor_group,n_instruments=N_INSTRUMENTS,z_size=Z_SIZE,ir_size=IR_SIZE) \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "\n",
    "\n",
    "BATCH_SIZE=16\n",
    "\n",
    "batched_trn_dataset= trn_dataset.shuffle(10000).batch(BATCH_SIZE*len(tf.config.list_physical_devices('GPU')),drop_remainder=True)\n",
    "batched_dist_trn_dataset = strategy.experimental_distribute_dataset(batched_trn_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "## training loop with adam\n",
    "\n",
    "\n",
    "checkpoint_path=f\"artefacts/ae_v3_checkpoint_family_{INSTRUMENT_FAMILY}\"\n",
    "try:\n",
    "    print(\"loading checkpoint\")\n",
    "    ae.load_weights(checkpoint_path)\n",
    "except:\n",
    "    print(\"couldn't load checkpoint\")\n",
    "    pass\n",
    "e=0\n",
    "\n",
    "while True:\n",
    "  batch_counter=0\n",
    "  epoch_loss=0   \n",
    "  for batch in batched_dist_trn_dataset:\n",
    "        it_loss=distributed_train_step(batch)\n",
    "        epoch_loss+=it_loss\n",
    "        if batch_counter % 10==0:\n",
    "            print(f\"batch nr {batch_counter}, loss: {it_loss.numpy()}\")\n",
    "\n",
    "        batch_counter+=1\n",
    "  \n",
    "\n",
    "  plotlosses.update({'loss': epoch_loss/batch_counter,})\n",
    "\n",
    "  plotlosses.send()\n",
    "\n",
    "  print(f\"summary nr: {e}\")\n",
    "\n",
    "\n",
    "  #play(tf.reshape(output[\"audio\"],(-1)))\n",
    "  #play(tf.reshape(output['audio_synth'],(-1)))\n",
    "  #play(tf.reshape(output['add'][\"signal\"],(-1))) \n",
    "    \n",
    "  #plt.plot(output[\"ir\"][0])\n",
    "  #plt.show()\n",
    "    \n",
    "  ae.save_weights(checkpoint_path)\n",
    "\n",
    "  e+=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(tf.reshape(batch[\"audio\"],(-1)))\n",
    "play(tf.reshape(output['add']['signal'],(-1))) \n",
    "play(tf.reshape(output['ir'],(-1))) \n",
    "play(tf.reshape(output['audio_synth'],(-1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pydash\n",
    "\n",
    "## validation \n",
    "\n",
    "val_dataset=list(val_dataset)\n",
    "\n",
    "VAL_LR=5e-3\n",
    "\n",
    "# order by velocity\n",
    "val_dataset=pydash.collections.sort_by(val_dataset,lambda x: x[\"velocity\"].numpy())\n",
    "\n",
    "#order by pitch\n",
    "val_dataset=pydash.collections.sort_by(val_dataset,lambda x: x[\"pitch\"].numpy())\n",
    "\n",
    "# group by instrument id\n",
    "val_dataset_by_instrument=pydash.collections.group_by(list(val_dataset),lambda x: str(x[\"instrument\"].numpy())+\" \"+str(x[\"velocity\"].numpy()))\n",
    "\n",
    "fit_iterations=100\n",
    "\n",
    "def rf2cf(row_form):\n",
    "    return {k:[s[k] for s in row_form] for k in row_form[0].keys()}\n",
    "    \n",
    "DEMO_NOTE_SAMPLES=int(0.2*SAMPLE_RATE)\n",
    "\n",
    "for instrument_set in val_dataset_by_instrument.values():    \n",
    "\n",
    "    val_optimizer = tf.keras.optimizers.Adam(learning_rate=VAL_LR)\n",
    "\n",
    "    # fit an embedding to highest and lowest note\n",
    "    \n",
    "    #fit_data=[instrument_set[int(len(instrument_set)/2)]]\n",
    "    \n",
    "    fit_data=[instrument_set[0],instrument_set[int(len(instrument_set)/2)],instrument_set[-1]]\n",
    "    \n",
    "    N_FIT_SAMPLES=len(fit_data)\n",
    "    \n",
    "    #convert to column form\n",
    "    fit_data = rf2cf(fit_data)\n",
    "        \n",
    "    fit_batch= next(iter(tf.data.Dataset.from_tensor_slices(fit_data).batch(N_FIT_SAMPLES)))\n",
    "    \n",
    "    fit_z=tf.Variable(tf.random.normal([1,INSTRUMENT_EMBEDDING_SIZE]))\n",
    "    fit_ir=tf.Variable(tf.concat([tf.zeros([1,1]),tf.zeros([1,IR_SIZE-1])],axis=-1))\n",
    "    \n",
    "    for i in range(fit_iterations):\n",
    "     \n",
    "        with tf.GradientTape() as tape:\n",
    "          fit_batch[\"z\"]=tf.tile(tf.tanh(fit_z),[N_FIT_SAMPLES,1])\n",
    "          fit_batch[\"ir\"]=tf.tile(tf.tanh(fit_ir),[N_FIT_SAMPLES,1])\n",
    "            \n",
    "          output=ae(fit_batch,training=False)\n",
    "\n",
    "          loss_value=spectral_loss(fit_batch[\"audio\"],output['audio_synth'])\n",
    "\n",
    "          gradients = tape.gradient(loss_value, [fit_z,fit_ir])\n",
    "          gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "            \n",
    "          print(loss_value)\n",
    "        \n",
    "        val_optimizer.apply_gradients(zip(gradients, [fit_z,fit_ir]))\n",
    "        \n",
    "    print(loss_value)\n",
    "           \n",
    "    play(tf.reshape(fit_batch[\"audio\"],(-1)))\n",
    "    play(tf.reshape(output['audio_synth'],(-1)))\n",
    "    play(tf.reshape(output['add'][\"signal\"],(-1)))\n",
    "    play(tf.reshape(fit_ir,(-1)))\n",
    "        \n",
    "    N_SCALE_SAMPLES=len(instrument_set)\n",
    "        \n",
    "    scale_data = rf2cf(instrument_set) \n",
    "    scale_batch= next(iter(tf.data.Dataset.from_tensor_slices(scale_data).batch(N_SCALE_SAMPLES)))\n",
    "    \n",
    "    play(tf.reshape(scale_batch[\"audio\"][:,:DEMO_NOTE_SAMPLES],(-1)))\n",
    "    \n",
    "    scale_batch[\"z\"] = tf.tile(tf.tanh(fit_z),[N_SCALE_SAMPLES,1])\n",
    "    scale_batch[\"ir\"] = tf.tile(fit_ir,[N_SCALE_SAMPLES,1])\n",
    "        \n",
    "    scale_batch_output=ae(scale_batch,training=False)\n",
    "    \n",
    "    play(tf.reshape(scale_batch_output['audio_synth'][:,:DEMO_NOTE_SAMPLES],(-1)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sgd training loop\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "checkpoint_path=f\"artefacts/ae_v2_checkpoint_family_{INSTRUMENT_FAMILY}\"\n",
    "try:\n",
    "    print(\"loading checkpoint\")\n",
    "    ae.load_weights(checkpoint_path)\n",
    "except:\n",
    "    print(\"couldn't load checkpoint\")\n",
    "    pass\n",
    "e=0\n",
    "\n",
    "\n",
    "#tf.debugging.experimental.enable_dump_debug_info(\"logdir\", tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\n",
    "\n",
    "while True:\n",
    "  batch_counter=0\n",
    "  epoch_loss=0   \n",
    "  for batch in batched_trn_dataset:\n",
    "    a = time.time()\n",
    "\n",
    "    batch_z=[]\n",
    "    batch_ir=[]\n",
    "\n",
    "    # ptr\n",
    "    for sample_index in range(batch[\"instrument\"].shape[0]):\n",
    "      instrument_id=batch[\"instrument\"][sample_index].numpy()\n",
    "      if instrument_id not in instrument_data:\n",
    "        instrument_data[instrument_id]={}\n",
    "        instrument_data[instrument_id][\"z\"]=tf.random.normal([INSTRUMENT_EMBEDDING_SIZE]).numpy()\n",
    "        instrument_data[instrument_id][\"ir\"]=tf.random.normal([IR_SIZE],mean=0,stddev=1e-6).numpy()\n",
    "\n",
    "      batch_z.append(instrument_data[instrument_id][\"z\"])\n",
    "      batch_ir.append(instrument_data[instrument_id][\"ir\"])   \n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "      batch_z_tf = tf.Variable(tf.stack(batch_z))\n",
    "      batch_ir_tf = tf.Variable(tf.stack(batch_ir)) \n",
    "\n",
    "      batch[\"z\"]=tf.tanh(batch_z_tf)\n",
    "      batch[\"ir\"]=tf.tanh(batch_ir_tf)\n",
    "\n",
    "      output=ae(batch,training=False)\n",
    "\n",
    "      loss_value=spectral_loss(batch[\"audio\"],output['audio_synth'])\n",
    "\n",
    "      gradients = tape.gradient(loss_value, [batch_z_tf,batch_ir_tf,*ae.trainable_variables])\n",
    "      gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "\n",
    "      epoch_loss+=loss_value.numpy()\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, [batch_z_tf,batch_ir_tf,*ae.trainable_variables]))\n",
    "\n",
    "    for sample_index in range(batch[\"instrument\"].shape[0]):\n",
    "      # TODO: average instrument embeddings per instrument\n",
    "      instrument_data[instrument_id][\"z\"]=batch_z_tf[sample_index].numpy()\n",
    "      instrument_data[instrument_id][\"ir\"]=batch_ir_tf[sample_index].numpy()\n",
    "\n",
    "    #print(f\"batch took {time.time()-a} s\")\n",
    "        \n",
    "    batch_counter+=1\n",
    "    \n",
    "  plotlosses.update({\n",
    "    'loss': epoch_loss/batch_counter,\n",
    "  })\n",
    "\n",
    "  plotlosses.send()\n",
    "\n",
    "  print(f\"summary nr: {e}\")\n",
    "\n",
    "  print(loss_value)\n",
    "  play(tf.reshape(batch[\"audio\"],(-1)))\n",
    "  play(tf.reshape(output['audio_synth'],(-1)))\n",
    "    \n",
    "  plt.plot(batch[\"ir\"][0])\n",
    "\n",
    "  ae.save_weights(checkpoint_path)\n",
    "\n",
    "  e+=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "slow instrument cloning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
