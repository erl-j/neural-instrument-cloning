{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3kBReFZb1eKk"
   },
   "outputs": [],
   "source": [
    "# imports and utils\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "import ddsp.training\n",
    "_AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "from IPython.display import Audio, display\n",
    "from livelossplot import PlotLosses\n",
    "import numpy as np\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import time\n",
    "import data\n",
    "\n",
    "# define constants\n",
    "N_SAMPLES=16000*4\n",
    "SAMPLE_RATE=16000\n",
    "SEED=1\n",
    "\n",
    "tf.random.set_seed(\n",
    "    SEED\n",
    ")\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "\n",
    "\n",
    "# define some utilis\n",
    "def play(audio):\n",
    "  display(Audio(audio,rate=SAMPLE_RATE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "USE_NSYNTH=False\n",
    "if USE_NSYNTH:\n",
    "    \n",
    "    class CustomNSynthTfds(ddsp.training.data.TfdsProvider):\n",
    "      \"\"\"Parses features in the TFDS NSynth dataset.\n",
    "      Unlike the default Nsynth data provider, this class keeps the the nsynth instrument metadata.\n",
    "\n",
    "      If running on Cloud, it is recommended you set `data_dir` to\n",
    "      'gs://tfds-data/datasets' to avoid unnecessary downloads.\n",
    "      \"\"\"\n",
    "\n",
    "      def __init__(self,\n",
    "                   name='nsynth/gansynth_subset.f0_and_loudness:2.3.3',\n",
    "                   split='train',\n",
    "                   data_dir='gs://tfds-data/datasets',\n",
    "                   sample_rate=16000,\n",
    "                   frame_rate=250,\n",
    "                   include_note_labels=True):\n",
    "        \"\"\"TfdsProvider constructor.\n",
    "        Args:\n",
    "          name: TFDS dataset name (with optional config and version).\n",
    "          split: Dataset split to use of the TFDS dataset.\n",
    "          data_dir: The directory to read the prepared NSynth dataset from. Defaults\n",
    "            to the public TFDS GCS bucket.\n",
    "          sample_rate: Sample rate of audio in the dataset.\n",
    "          frame_rate: Frame rate of features in the dataset.\n",
    "          include_note_labels: Return dataset without note-level labels\n",
    "            (pitch, instrument).\n",
    "        \"\"\"\n",
    "        self._include_note_labels = include_note_labels\n",
    "\n",
    "        super().__init__(name, split, data_dir, sample_rate, frame_rate)\n",
    "\n",
    "      def get_dataset(self, shuffle=True):\n",
    "        \"\"\"Returns dataset with slight restructuring of feature dictionary.\"\"\"\n",
    "        def preprocess_ex(ex):\n",
    "          ex_out = {\n",
    "              'audio':\n",
    "                  ex['audio'],\n",
    "              'f0_hz':\n",
    "                  ex['f0']['hz'],\n",
    "              'f0_confidence':\n",
    "                  ex['f0']['confidence'],\n",
    "              'loudness_db':\n",
    "                  ex['loudness']['db'],\n",
    "          }\n",
    "          if self._include_note_labels:\n",
    "            ex_out.update({\n",
    "                'pitch':\n",
    "                    ex['pitch'],\n",
    "                'velocity':\n",
    "                    ex['velocity'],\n",
    "                'instrument_source':\n",
    "                    ex['instrument']['source'],\n",
    "                'instrument_family':\n",
    "                    ex['instrument']['family'],\n",
    "                'instrument':\n",
    "                    ex['instrument']['label'],\n",
    "            })\n",
    "          return ex_out\n",
    "\n",
    "        dataset = super().get_dataset(shuffle)\n",
    "        dataset = dataset.map(preprocess_ex, num_parallel_calls=_AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    tfds.load(\"nsynth/gansynth_subset.f0_and_loudness\",split=\"train\", try_gcs=False,download=True) \n",
    "    trn_data_provider = CustomNSynthTfds(data_dir=\"/root/tensorflow_datasets/\",split=\"train\")\n",
    "\n",
    "    tfds.load(\"nsynth/gansynth_subset.f0_and_loudness\",split=\"valid\", try_gcs=False,download=True) \n",
    "    val_data_provider = CustomNSynthTfds(data_dir=\"/root/tensorflow_datasets/\",split=\"valid\")\n",
    "    \n",
    "    # take a batch of flute sounds\n",
    "    #dataset = data_provider.get_dataset()\n",
    "\n",
    "    # take only acoustic sounds\n",
    "    #dataset=dataset.filter(lambda x: x[\"instrument_source\"]==0)\n",
    "\n",
    "    # take only flute sounds\n",
    "    #dataset=dataset.filter(lambda x: x[\"instrument_family\"]==2)\n",
    "\n",
    "    # flutes\n",
    "    # 2965 samples\n",
    "    # 36 instruments\n",
    "    # 5 velocities \n",
    "    # 61 pitches\n",
    "\n",
    "    #test_batch=next(iter(dataset.batch(4))) \n",
    "\n",
    "    #play(tf.reshape(test_batch[\"audio\"],(-1)))\n",
    "\n",
    "    # TODO: CLEAN UP DATASET. REMOVE WHERE CREPE IS UNCERTAIN.\n",
    "\n",
    "    def crepe_is_certain(x):\n",
    "        is_playing = tf.cast(x[\"loudness_db\"]>-100.0,dtype=tf.float32)\n",
    "        average_certainty=tf.reduce_sum(x[\"f0_confidence\"]*is_playing)/tf.reduce_sum(is_playing)\n",
    "        return average_certainty\n",
    "\n",
    "    '''\n",
    "    for s in range(1):\n",
    "\n",
    "        print(\"###\")\n",
    "\n",
    "        sample = next(iter(dataset))\n",
    "        play(sample[\"audio\"])\n",
    "\n",
    "        print(crepe_is_certain(sample))\n",
    "\n",
    "        plt.plot(sample[\"loudness_db\"])\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(sample[\"f0_hz\"])\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(sample[\"f0_confidence\"])\n",
    "        plt.show()\n",
    "\n",
    "    '''\n",
    "\n",
    "    INSTRUMENT_FAMILY=2\n",
    "\n",
    "    def preprocess_dataset(dataset):\n",
    "        if INSTRUMENT_FAMILY!=\"all\":\n",
    "            dataset=dataset.filter(lambda x: x[\"instrument_family\"]==INSTRUMENT_FAMILY)\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    trn_dataset = preprocess_dataset(trn_data_provider.get_dataset())\n",
    "    val_dataset = preprocess_dataset(val_data_provider.get_dataset())\n",
    "\n",
    "    # take only flute sounds\n",
    "\n",
    "\n",
    "else:\n",
    "    INSTRUMENT_FAMILY=\"solos-violin-clean\"\n",
    "    \n",
    "    trn_data_provider=data.MultiTFRecordProvider(f\"datasets/{INSTRUMENT_FAMILY}/tfr/trn/*\")\n",
    "    val_data_provider=data.MultiTFRecordProvider(f\"datasets/{INSTRUMENT_FAMILY}/tfr/val/*\")\n",
    "    \n",
    "    trn_dataset= trn_data_provider.get_dataset()\n",
    "    val_dataset=val_data_provider.get_dataset()\n",
    "    \n",
    "    def preprocess_dataset(dataset):\n",
    "        dataset=dataset.filter(lambda x: tf.math.count_nonzero(x[\"audio\"])>0)\n",
    "        return dataset\n",
    "    \n",
    "    trn_dataset = preprocess_dataset(trn_dataset)\n",
    "    val_dataset = preprocess_dataset(val_dataset)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Kj83Q3YrDEeT"
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "N_NOISE_MAGNITUDES=64\n",
    "N_HARMONICS=64\n",
    "\n",
    "\n",
    "class CustomReverb(ddsp.processors.Processor):\n",
    "\n",
    "    def __init__(self,name='reverb'):\n",
    "        \"\"\"Takes neural network outputs directly as the impulse response.\n",
    "        Args:\n",
    "          trainable: Learn the impulse_response as a single variable for the entire\n",
    "            dataset.\n",
    "          reverb_length: Length of the impulse response. Only used if\n",
    "            trainable=True.\n",
    "          add_dry: Add dry signal to reverberated signal on output.\n",
    "          name: Name of processor module.\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "    \n",
    "    def get_controls(self, audio, ir=None):\n",
    "        \"\"\"Convert decoder outputs into ir response.\n",
    "        Args:\n",
    "          audio: Dry audio. 2-D Tensor of shape [batch, n_samples].\n",
    "          ir: 3-D Tensor of shape [batch, ir_size, 1] or 2D Tensor of shape\n",
    "            [batch, ir_size].\n",
    "        Returns:\n",
    "          controls: Dictionary of effect controls.\n",
    "        \"\"\"\n",
    "        return {'audio': audio, 'ir': ir}\n",
    "\n",
    "    def get_signal(self, audio, ir):\n",
    "        \"\"\"Apply impulse response.\n",
    "        Args:\n",
    "          audio: Dry audio, 2-D Tensor of shape [batch, n_samples].\n",
    "          ir: 3-D Tensor of shape [batch, ir_size, 1] or 2D Tensor of shape\n",
    "            [batch, ir_size].\n",
    "        Returns:\n",
    "          tensor of shape [batch, n_samples]\n",
    "        \"\"\"\n",
    "        wet = ddsp.core.fft_convolve(audio, ir, padding='same', delay_compensation=0)\n",
    "        return wet\n",
    "\n",
    "class LGD(ddsp.training.models.Model):\n",
    "\n",
    "    def __init__(self, n_out,  rnn_channels=256, rnn_type='gru', ch=512, layers_per_stack=0, name=None):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        def stack():\n",
    "            return ddsp.training.nn.FcStack(ch, layers_per_stack)\n",
    "\n",
    "        self.input_keys = [\"f0_hz\", \"ld_scaled\"]\n",
    "\n",
    "        self.n_out = n_out\n",
    "        self.input_stacks = [stack() for k in self.input_keys]\n",
    "\n",
    "        self.rnn = ddsp.training.nn.Rnn(rnn_channels, rnn_type)\n",
    "        self.out_stack = stack()\n",
    "        self.dense_out = ddsp.training.nn.Fc(self.n_out)\n",
    "\n",
    "    def call(self, conditioning):\n",
    "\n",
    "        instrument_representation = conditioning[\"z\"]\n",
    "\n",
    "        feature_len = conditioning[\"f0_scaled\"].shape[1]\n",
    "\n",
    "        instrument_representation = tf.repeat(\n",
    "            instrument_representation[:, None, ...], feature_len, axis=1)\n",
    "\n",
    "        inputs = [conditioning[k] for k in self.input_keys]\n",
    "\n",
    "        inputs = [stack(x) for stack, x in zip(self.input_stacks, inputs)]\n",
    "\n",
    "        x = tf.concat(inputs, axis=(-1))\n",
    "        x = tf.concat([x, instrument_representation], axis=2)\n",
    "        x = self.rnn(x)\n",
    "        x = tf.concat((inputs + [x]), axis=(-1))\n",
    "        x = self.out_stack(x)\n",
    "        out = self.dense_out(x)\n",
    "        \n",
    "        \n",
    "        loudness=out[:,:,-1:]\n",
    "        noise_magnitudes = out[:,:,:N_NOISE_MAGNITUDES]\n",
    "        harmonic_distribution = out[:,:,-N_HARMONICS:-1]\n",
    "            \n",
    "        return {\"amps\":loudness,\"harmonic_distribution\":harmonic_distribution,\"magnitudes\":noise_magnitudes}\n",
    "    \n",
    "preprocessor=ddsp.training.preprocessing.F0LoudnessPreprocessor()\n",
    "decoder = LGD(n_out=N_NOISE_MAGNITUDES+N_HARMONICS+1)\n",
    "harmonic_synth = ddsp.synths.Harmonic(\n",
    "    n_samples=N_SAMPLES, sample_rate=SAMPLE_RATE, name='harmonic')\n",
    "\n",
    "filtered_noise = ddsp.synths.FilteredNoise(\n",
    "    n_samples=N_SAMPLES, window_size=0, initial_bias=-10.0, name='noise')\n",
    "reverb = ddsp.effects.Reverb(name=\"reverb\",trainable=False)\n",
    "add = ddsp.processors.Add(name='add')\n",
    "\n",
    "dag = [\n",
    "  (harmonic_synth, ['amps', 'harmonic_distribution', 'f0_hz']),\n",
    "  (filtered_noise, ['magnitudes']),\n",
    "  (add, ['harmonic/signal', 'noise/signal']),\n",
    "  (reverb, [\"add/signal\",\"ir\"])\n",
    "]\n",
    "\n",
    "processor_group=ddsp.processors.ProcessorGroup(dag=dag)\n",
    "\n",
    "spectral_loss = ddsp.losses.SpectralLoss(loss_type='L1',\n",
    "                                              mag_weight=1.0,\n",
    "                                              logmag_weight=1.0)\n",
    "\n",
    "\n",
    "class MultiInstrumentAutoencoder(ddsp.training.models.autoencoder.Autoencoder):\n",
    "    def __init__(self,\n",
    "               preprocessor=None,\n",
    "               encoder=None,\n",
    "               decoder=None,\n",
    "               processor_group=None,\n",
    "               losses=None,\n",
    "               n_instruments=None,\n",
    "               z_size=None,\n",
    "               ir_size=None,\n",
    "               **kwargs):\n",
    "        super().__init__(preprocessor,encoder,decoder,processor_group,losses,**kwargs)\n",
    "        \n",
    "        self.n_instruments=n_instruments\n",
    "        self.instrument_z = tf.Variable(tf.random.normal([n_instruments,z_size]))\n",
    "        self.instrument_ir = tf.Variable(tf.concat([tf.ones([n_instruments,1]),tf.zeros([n_instruments,ir_size-1])],axis=-1))\n",
    "        self.instrument_id2idx={}\n",
    "        \n",
    "    def call(self, batch, training=True):\n",
    "        instrument_idxs=[]\n",
    " \n",
    "        for sample_index in range(batch[\"instrument\"].shape[0]):\n",
    "          instrument_id=str(batch[\"instrument\"][sample_index])\n",
    "          if instrument_id not in self.instrument_id2idx:\n",
    "              self.instrument_id2idx[instrument_id]=len(self.instrument_id2idx.keys())\n",
    "          instrument_idxs.append(int(self.instrument_id2idx[instrument_id]))\n",
    "  \n",
    "        with tf.GradientTape() as tape:\n",
    "          batch[\"z\"]=tf.tanh(tf.gather(self.instrument_z,instrument_idxs))\n",
    "          batch[\"ir\"]=tf.tanh(tf.gather(self.instrument_ir,instrument_idxs))     \n",
    "        return super().call(batch,training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wJUeq9Kmwbga",
    "outputId": "d27f1547-a061-4bf7-d3d6-f5715199fb3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7')\n",
      "loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "Z_SIZE=256\n",
    "\n",
    "N_INSTRUMENTS=1050\n",
    "\n",
    "IR_SIZE=int(SAMPLE_RATE*1.0)\n",
    "\n",
    "checkpoint_path=f\"artefacts/ae_v3_checkpoint_family_{INSTRUMENT_FAMILY}\"\n",
    "\n",
    "plotlosses = PlotLosses()\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
    "\n",
    "def train_step(inputs):\n",
    "     \n",
    "    with tf.GradientTape() as tape:\n",
    "    \n",
    "        a = time.time()\n",
    "\n",
    "        output=ae(inputs,training=False)\n",
    "\n",
    "        loss_value=spectral_loss(inputs[\"audio\"],output['audio_synth'])\n",
    "\n",
    "        gradients = tape.gradient(loss_value, [*ae.trainable_variables])\n",
    "\n",
    "        #grad_is_nan=False\n",
    "        #for g in gradients:\n",
    "        #    grad_is_nan=grad_is_nan or tf.math.reduce_any(tf.math.is_nan(g))\n",
    "        \n",
    "\n",
    "        # if not grad_is_nan:\n",
    "        optimizer.apply_gradients(zip(gradients, [*ae.trainable_variables]))\n",
    "        \n",
    "    return loss_value\n",
    "    \n",
    "\n",
    "@tf.function\n",
    "def distributed_train_step(dataset_inputs):\n",
    "    per_replica_losses=strategy.run(train_step,args=(dataset_inputs,))\n",
    "    return strategy.reduce(tf.distribute.ReduceOp.SUM,per_replica_losses,axis=None)\n",
    "\n",
    "with strategy.scope():\n",
    "    ae = MultiInstrumentAutoencoder(preprocessor=preprocessor,decoder=decoder,processor_group=processor_group,n_instruments=N_INSTRUMENTS,z_size=Z_SIZE,ir_size=IR_SIZE) \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        print(\"loading checkpoint\")\n",
    "        ae.load_weights(checkpoint_path)\n",
    "    except:\n",
    "        print(\"couldn't load checkpoint\")\n",
    "        pass\n",
    "\n",
    "\n",
    "BATCH_SIZE=16\n",
    "\n",
    "batched_trn_dataset= trn_dataset.shuffle(10000).batch(BATCH_SIZE*len(tf.config.list_physical_devices('GPU')),drop_remainder=True)\n",
    "batched_dist_trn_dataset = strategy.experimental_distribute_dataset(batched_trn_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss\n",
      "\tloss             \t (min:   50.073, max:   50.073, cur:   50.073)\n",
      "summary nr: 0\n",
      "batch nr 0, loss: 50.12508773803711\n",
      "batch nr 10, loss: 46.71537780761719\n",
      "batch nr 20, loss: 47.93402862548828\n",
      "batch nr 30, loss: 51.723968505859375\n",
      "batch nr 40, loss: 46.1602897644043\n",
      "batch nr 50, loss: 56.73405075073242\n",
      "batch nr 60, loss: 51.810218811035156\n",
      "batch nr 70, loss: 46.3394889831543\n",
      "batch nr 80, loss: 49.36194610595703\n",
      "batch nr 90, loss: 49.7179069519043\n",
      "batch nr 100, loss: 55.59546661376953\n",
      "batch nr 110, loss: 45.16313934326172\n"
     ]
    }
   ],
   "source": [
    "## training loop with adam\n",
    "\n",
    "\n",
    "e=0\n",
    "\n",
    "while True:\n",
    "  batch_counter=0\n",
    "  epoch_loss=0   \n",
    "  for batch in batched_dist_trn_dataset:\n",
    "        it_loss=distributed_train_step(batch)\n",
    "        epoch_loss+=it_loss\n",
    "        if batch_counter % 10==0:\n",
    "            print(f\"batch nr {batch_counter}, loss: {it_loss.numpy()}\")\n",
    "\n",
    "        batch_counter+=1\n",
    "  \n",
    "\n",
    "  plotlosses.update({'loss': epoch_loss/batch_counter,})\n",
    "\n",
    "  plotlosses.send()\n",
    "\n",
    "  print(f\"summary nr: {e}\")\n",
    "\n",
    "\n",
    "  #play(tf.reshape(output[\"audio\"],(-1)))\n",
    "  #play(tf.reshape(output['audio_synth'],(-1)))\n",
    "  #play(tf.reshape(output['add'][\"signal\"],(-1))) \n",
    "    \n",
    "  #plt.plot(output[\"ir\"][0])\n",
    "  #plt.show()\n",
    "    \n",
    "  ae.save_weights(checkpoint_path)\n",
    "\n",
    "  e+=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(tf.reshape(batch[\"audio\"],(-1)))\n",
    "play(tf.reshape(output['add']['signal'],(-1))) \n",
    "play(tf.reshape(output['ir'],(-1))) \n",
    "play(tf.reshape(output['audio_synth'],(-1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pydash\n",
    "\n",
    "## validation \n",
    "\n",
    "val_dataset=list(val_dataset)\n",
    "\n",
    "VAL_LR=5e-3\n",
    "\n",
    "# order by velocity\n",
    "val_dataset=pydash.collections.sort_by(val_dataset,lambda x: x[\"velocity\"].numpy())\n",
    "\n",
    "#order by pitch\n",
    "val_dataset=pydash.collections.sort_by(val_dataset,lambda x: x[\"pitch\"].numpy())\n",
    "\n",
    "# group by instrument id\n",
    "val_dataset_by_instrument=pydash.collections.group_by(list(val_dataset),lambda x: str(x[\"instrument\"].numpy())+\" \"+str(x[\"velocity\"].numpy()))\n",
    "\n",
    "fit_iterations=100\n",
    "\n",
    "def rf2cf(row_form):\n",
    "    return {k:[s[k] for s in row_form] for k in row_form[0].keys()}\n",
    "    \n",
    "DEMO_NOTE_SAMPLES=int(0.2*SAMPLE_RATE)\n",
    "\n",
    "for instrument_set in val_dataset_by_instrument.values():    \n",
    "\n",
    "    val_optimizer = tf.keras.optimizers.Adam(learning_rate=VAL_LR)\n",
    "\n",
    "    # fit an embedding to highest and lowest note\n",
    "    \n",
    "    #fit_data=[instrument_set[int(len(instrument_set)/2)]]\n",
    "    \n",
    "    fit_data=[instrument_set[0],instrument_set[int(len(instrument_set)/2)],instrument_set[-1]]\n",
    "    \n",
    "    N_FIT_SAMPLES=len(fit_data)\n",
    "    \n",
    "    #convert to column form\n",
    "    fit_data = rf2cf(fit_data)\n",
    "        \n",
    "    fit_batch= next(iter(tf.data.Dataset.from_tensor_slices(fit_data).batch(N_FIT_SAMPLES)))\n",
    "    \n",
    "    fit_z=tf.Variable(tf.random.normal([1,INSTRUMENT_EMBEDDING_SIZE]))\n",
    "    fit_ir=tf.Variable(tf.concat([tf.zeros([1,1]),tf.zeros([1,IR_SIZE-1])],axis=-1))\n",
    "    \n",
    "    for i in range(fit_iterations):\n",
    "     \n",
    "        with tf.GradientTape() as tape:\n",
    "          fit_batch[\"z\"]=tf.tile(tf.tanh(fit_z),[N_FIT_SAMPLES,1])\n",
    "          fit_batch[\"ir\"]=tf.tile(tf.tanh(fit_ir),[N_FIT_SAMPLES,1])\n",
    "            \n",
    "          output=ae(fit_batch,training=False)\n",
    "\n",
    "          loss_value=spectral_loss(fit_batch[\"audio\"],output['audio_synth'])\n",
    "\n",
    "          gradients = tape.gradient(loss_value, [fit_z,fit_ir])\n",
    "          gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "            \n",
    "          print(loss_value)\n",
    "        \n",
    "        val_optimizer.apply_gradients(zip(gradients, [fit_z,fit_ir]))\n",
    "        \n",
    "    print(loss_value)\n",
    "           \n",
    "    play(tf.reshape(fit_batch[\"audio\"],(-1)))\n",
    "    play(tf.reshape(output['audio_synth'],(-1)))\n",
    "    play(tf.reshape(output['add'][\"signal\"],(-1)))\n",
    "    play(tf.reshape(fit_ir,(-1)))\n",
    "        \n",
    "    N_SCALE_SAMPLES=len(instrument_set)\n",
    "        \n",
    "    scale_data = rf2cf(instrument_set) \n",
    "    scale_batch= next(iter(tf.data.Dataset.from_tensor_slices(scale_data).batch(N_SCALE_SAMPLES)))\n",
    "    \n",
    "    play(tf.reshape(scale_batch[\"audio\"][:,:DEMO_NOTE_SAMPLES],(-1)))\n",
    "    \n",
    "    scale_batch[\"z\"] = tf.tile(tf.tanh(fit_z),[N_SCALE_SAMPLES,1])\n",
    "    scale_batch[\"ir\"] = tf.tile(fit_ir,[N_SCALE_SAMPLES,1])\n",
    "        \n",
    "    scale_batch_output=ae(scale_batch,training=False)\n",
    "    \n",
    "    play(tf.reshape(scale_batch_output['audio_synth'][:,:DEMO_NOTE_SAMPLES],(-1)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sgd training loop\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "checkpoint_path=f\"artefacts/ae_v2_checkpoint_family_{INSTRUMENT_FAMILY}\"\n",
    "try:\n",
    "    print(\"loading checkpoint\")\n",
    "    ae.load_weights(checkpoint_path)\n",
    "except:\n",
    "    print(\"couldn't load checkpoint\")\n",
    "    pass\n",
    "e=0\n",
    "\n",
    "\n",
    "#tf.debugging.experimental.enable_dump_debug_info(\"logdir\", tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\n",
    "\n",
    "while True:\n",
    "  batch_counter=0\n",
    "  epoch_loss=0   \n",
    "  for batch in batched_trn_dataset:\n",
    "    a = time.time()\n",
    "\n",
    "    batch_z=[]\n",
    "    batch_ir=[]\n",
    "\n",
    "    # ptr\n",
    "    for sample_index in range(batch[\"instrument\"].shape[0]):\n",
    "      instrument_id=batch[\"instrument\"][sample_index].numpy()\n",
    "      if instrument_id not in instrument_data:\n",
    "        instrument_data[instrument_id]={}\n",
    "        instrument_data[instrument_id][\"z\"]=tf.random.normal([INSTRUMENT_EMBEDDING_SIZE]).numpy()\n",
    "        instrument_data[instrument_id][\"ir\"]=tf.random.normal([IR_SIZE],mean=0,stddev=1e-6).numpy()\n",
    "\n",
    "      batch_z.append(instrument_data[instrument_id][\"z\"])\n",
    "      batch_ir.append(instrument_data[instrument_id][\"ir\"])   \n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "      batch_z_tf = tf.Variable(tf.stack(batch_z))\n",
    "      batch_ir_tf = tf.Variable(tf.stack(batch_ir)) \n",
    "\n",
    "      batch[\"z\"]=tf.tanh(batch_z_tf)\n",
    "      batch[\"ir\"]=tf.tanh(batch_ir_tf)\n",
    "\n",
    "      output=ae(batch,training=False)\n",
    "\n",
    "      loss_value=spectral_loss(batch[\"audio\"],output['audio_synth'])\n",
    "\n",
    "      gradients = tape.gradient(loss_value, [batch_z_tf,batch_ir_tf,*ae.trainable_variables])\n",
    "      gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "\n",
    "      epoch_loss+=loss_value.numpy()\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, [batch_z_tf,batch_ir_tf,*ae.trainable_variables]))\n",
    "\n",
    "    for sample_index in range(batch[\"instrument\"].shape[0]):\n",
    "      # TODO: average instrument embeddings per instrument\n",
    "      instrument_data[instrument_id][\"z\"]=batch_z_tf[sample_index].numpy()\n",
    "      instrument_data[instrument_id][\"ir\"]=batch_ir_tf[sample_index].numpy()\n",
    "\n",
    "    #print(f\"batch took {time.time()-a} s\")\n",
    "        \n",
    "    batch_counter+=1\n",
    "    \n",
    "  plotlosses.update({\n",
    "    'loss': epoch_loss/batch_counter,\n",
    "  })\n",
    "\n",
    "  plotlosses.send()\n",
    "\n",
    "  print(f\"summary nr: {e}\")\n",
    "\n",
    "  print(loss_value)\n",
    "  play(tf.reshape(batch[\"audio\"],(-1)))\n",
    "  play(tf.reshape(output['audio_synth'],(-1)))\n",
    "    \n",
    "  plt.plot(batch[\"ir\"][0])\n",
    "\n",
    "  ae.save_weights(checkpoint_path)\n",
    "\n",
    "  e+=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "slow instrument cloning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
