{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3kBReFZb1eKk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# imports and utils\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "import ddsp.training\n",
    "_AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "from IPython.display import Audio, display\n",
    "from livelossplot import PlotLosses\n",
    "import numpy as np\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import time\n",
    "# define constants\n",
    "N_SAMPLES=16000*4\n",
    "SAMPLE_RATE=16000\n",
    "\n",
    "\n",
    "SEED=1\n",
    "\n",
    "tf.random.set_seed(\n",
    "    SEED\n",
    ")\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "\n",
    "\n",
    "# define some utilis\n",
    "def play(audio):\n",
    "  display(Audio(audio,rate=SAMPLE_RATE))\n",
    "\n",
    "class CustomNSynthTfds(ddsp.training.data.TfdsProvider):\n",
    "  \"\"\"Parses features in the TFDS NSynth dataset.\n",
    "  If running on Cloud, it is recommended you set `data_dir` to\n",
    "  'gs://tfds-data/datasets' to avoid unnecessary downloads.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               name='nsynth/gansynth_subset.f0_and_loudness:2.3.3',\n",
    "               split='train',\n",
    "               data_dir='gs://tfds-data/datasets',\n",
    "               sample_rate=16000,\n",
    "               frame_rate=250,\n",
    "               include_note_labels=True):\n",
    "    \"\"\"TfdsProvider constructor.\n",
    "    Args:\n",
    "      name: TFDS dataset name (with optional config and version).\n",
    "      split: Dataset split to use of the TFDS dataset.\n",
    "      data_dir: The directory to read the prepared NSynth dataset from. Defaults\n",
    "        to the public TFDS GCS bucket.\n",
    "      sample_rate: Sample rate of audio in the dataset.\n",
    "      frame_rate: Frame rate of features in the dataset.\n",
    "      include_note_labels: Return dataset without note-level labels\n",
    "        (pitch, instrument).\n",
    "    \"\"\"\n",
    "    self._include_note_labels = include_note_labels\n",
    "\n",
    "    super().__init__(name, split, data_dir, sample_rate, frame_rate)\n",
    "\n",
    "  def get_dataset(self, shuffle=True):\n",
    "    \"\"\"Returns dataset with slight restructuring of feature dictionary.\"\"\"\n",
    "    def preprocess_ex(ex):\n",
    "      ex_out = {\n",
    "          'audio':\n",
    "              ex['audio'],\n",
    "          'f0_hz':\n",
    "              ex['f0']['hz'],\n",
    "          'f0_confidence':\n",
    "              ex['f0']['confidence'],\n",
    "          'loudness_db':\n",
    "              ex['loudness']['db'],\n",
    "      }\n",
    "      if self._include_note_labels:\n",
    "        ex_out.update({\n",
    "            'pitch':\n",
    "                ex['pitch'],\n",
    "            'velocity':\n",
    "                ex['velocity'],\n",
    "            'instrument_source':\n",
    "                ex['instrument']['source'],\n",
    "            'instrument_family':\n",
    "                ex['instrument']['family'],\n",
    "            'instrument':\n",
    "                ex['instrument']['label'],\n",
    "        })\n",
    "      return ex_out\n",
    "\n",
    "    dataset = super().get_dataset(shuffle)\n",
    "    dataset = dataset.map(preprocess_ex, num_parallel_calls=_AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "tfds.load(\"nsynth/gansynth_subset.f0_and_loudness\",split=\"train\", try_gcs=False,download=True) \n",
    "trn_data_provider = CustomNSynthTfds(data_dir=\"/root/tensorflow_datasets/\",split=\"train\")\n",
    "\n",
    "tfds.load(\"nsynth/gansynth_subset.f0_and_loudness\",split=\"valid\", try_gcs=False,download=True) \n",
    "val_data_provider = CustomNSynthTfds(data_dir=\"/root/tensorflow_datasets/\",split=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Kj83Q3YrDEeT"
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "N_NOISE_MAGNITUDES=64\n",
    "N_HARMONICS=64\n",
    "\n",
    "class LGD(ddsp.training.models.Model):\n",
    "\n",
    "    def __init__(self, n_out,  rnn_channels=256, rnn_type='gru', ch=512, layers_per_stack=0, name=None):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        def stack():\n",
    "            return ddsp.training.nn.FcStack(ch, layers_per_stack)\n",
    "\n",
    "        self.input_keys = [\"f0_hz\", \"ld_scaled\"]\n",
    "\n",
    "        self.n_out = n_out\n",
    "        self.input_stacks = [stack() for k in self.input_keys]\n",
    "\n",
    "        self.rnn = ddsp.training.nn.Rnn(rnn_channels, rnn_type)\n",
    "        self.out_stack = stack()\n",
    "        self.dense_out = ddsp.training.nn.Fc(self.n_out)\n",
    "\n",
    "    def call(self, conditioning):\n",
    "\n",
    "\n",
    "        instrument_representation = conditioning[\"z\"]\n",
    "\n",
    "        feature_len = conditioning[\"f0_scaled\"].shape[1]\n",
    "\n",
    "        instrument_representation = tf.repeat(\n",
    "            instrument_representation[:, None, ...], feature_len, axis=1)\n",
    "\n",
    "        inputs = [conditioning[k] for k in self.input_keys]\n",
    "\n",
    "        inputs = [stack(x) for stack, x in zip(self.input_stacks, inputs)]\n",
    "\n",
    "        x = tf.concat(inputs, axis=(-1))\n",
    "        x = tf.concat([x, instrument_representation], axis=2)\n",
    "        x = self.rnn(x)\n",
    "        x = tf.concat((inputs + [x]), axis=(-1))\n",
    "        x = self.out_stack(x)\n",
    "        out = self.dense_out(x)\n",
    "        \n",
    "        \n",
    "        loudness=out[:,:,-1:]\n",
    "        noise_magnitudes = out[:,:,:N_NOISE_MAGNITUDES]\n",
    "        harmonic_distribution = out[:,:,-N_HARMONICS:-1]\n",
    "            \n",
    "        return {\"amps\":loudness,\"harmonic_distribution\":harmonic_distribution,\"magnitudes\":noise_magnitudes}\n",
    "    \n",
    "preprocessor=ddsp.training.preprocessing.F0LoudnessPreprocessor()\n",
    "decoder = LGD(n_out=N_NOISE_MAGNITUDES+N_HARMONICS+1)\n",
    "harmonic_synth = ddsp.synths.Harmonic(\n",
    "    n_samples=N_SAMPLES, sample_rate=SAMPLE_RATE, name='harmonic')\n",
    "\n",
    "filtered_noise = ddsp.synths.FilteredNoise(\n",
    "    n_samples=N_SAMPLES, window_size=0, initial_bias=-10.0, name='noise')\n",
    "reverb = ddsp.effects.Reverb(trainable=False,reverb_length=N_SAMPLES, name='reverb')\n",
    "add = ddsp.processors.Add(name='add')\n",
    "\n",
    "dag = [\n",
    "  (harmonic_synth, ['amps', 'harmonic_distribution', 'f0_hz']),\n",
    "  (filtered_noise, ['magnitudes']),\n",
    "  (add, ['harmonic/signal', 'noise/signal']),\n",
    "  (reverb, [\"add/signal\",\"ir\"])\n",
    "]\n",
    "\n",
    "processor_group=ddsp.processors.ProcessorGroup(dag=dag)\n",
    "\n",
    "spectral_loss = ddsp.losses.SpectralLoss(loss_type='L1',\n",
    "                                              mag_weight=1.0,\n",
    "                                              logmag_weight=1.0)\n",
    "\n",
    "ae = ddsp.training.models.autoencoder.Autoencoder(preprocessor=preprocessor,decoder=decoder,processor_group=processor_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 62
    },
    "id": "piuIUekLQRT7",
    "outputId": "8e462565-e933-416f-c766-9e0ff17f2afb"
   },
   "outputs": [],
   "source": [
    "# take a batch of flute sounds\n",
    "#dataset = data_provider.get_dataset()\n",
    "\n",
    "# take only acoustic sounds\n",
    "#dataset=dataset.filter(lambda x: x[\"instrument_source\"]==0)\n",
    "\n",
    "# take only flute sounds\n",
    "#dataset=dataset.filter(lambda x: x[\"instrument_family\"]==2)\n",
    "\n",
    "# flutes\n",
    "# 2965 samples\n",
    "# 36 instruments\n",
    "# 5 velocities \n",
    "# 61 pitches\n",
    "\n",
    "#test_batch=next(iter(dataset.batch(4))) \n",
    "\n",
    "#play(tf.reshape(test_batch[\"audio\"],(-1)))\n",
    "\n",
    "# TODO: CLEAN UP DATASET. REMOVE WHERE CREPE IS UNCERTAIN.\n",
    "\n",
    "def crepe_is_certain(x):\n",
    "    is_playing = tf.cast(x[\"loudness_db\"]>-100.0,dtype=tf.float32)\n",
    "    average_certainty=tf.reduce_sum(x[\"f0_confidence\"]*is_playing)/tf.reduce_sum(is_playing)\n",
    "    return average_certainty\n",
    "\n",
    "'''\n",
    "for s in range(1):\n",
    "    \n",
    "    print(\"###\")\n",
    "    \n",
    "    sample = next(iter(dataset))\n",
    "    play(sample[\"audio\"])\n",
    "    \n",
    "    print(crepe_is_certain(sample))\n",
    "    \n",
    "    plt.plot(sample[\"loudness_db\"])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(sample[\"f0_hz\"])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(sample[\"f0_confidence\"])\n",
    "    plt.show()\n",
    "    \n",
    "'''\n",
    "\n",
    "INSTRUMENT_FAMILY=4\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    dataset=dataset.filter(lambda x: x[\"instrument_family\"]==INSTRUMENT_FAMILY)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "trn_dataset = preprocess_dataset(trn_data_provider.get_dataset())\n",
    "val_dataset = preprocess_dataset(val_data_provider.get_dataset())\n",
    "\n",
    "# take only flute sounds\n",
    "    \n",
    "BATCH_SIZE=16\n",
    "\n",
    "batched_trn_dataset= trn_dataset.shuffle(10000).batch(BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wJUeq9Kmwbga",
    "outputId": "d27f1547-a061-4bf7-d3d6-f5715199fb3b"
   },
   "outputs": [],
   "source": [
    "INSTRUMENT_EMBEDDING_SIZE=256\n",
    "\n",
    "IR_SIZE=int(SAMPLE_RATE*1)\n",
    "\n",
    "instrument_data = {}\n",
    "\n",
    "plotlosses = PlotLosses()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "checkpoint_path=f\"artefacts/ae_v2_checkpoint_family_{INSTRUMENT_FAMILY}\"\n",
    "try:\n",
    "    print(\"loading checkpoint\")\n",
    "    ae.load_weights(checkpoint_path)\n",
    "except:\n",
    "    print(\"couldn't load checkpoint\")\n",
    "    pass\n",
    "e=0\n",
    "\n",
    "\n",
    "#tf.debugging.experimental.enable_dump_debug_info(\"logdir\", tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\n",
    "\n",
    "while True:\n",
    "  batch_counter=0\n",
    "  epoch_loss=0   \n",
    "  for batch in batched_trn_dataset:\n",
    "    with tf.profiler.experimental.Trace('train', step_num=batch_counter, _r=1):\n",
    "    \n",
    "        batch_z=[]\n",
    "        batch_ir=[]\n",
    "\n",
    "        # ptr\n",
    "        for sample_index in range(batch[\"instrument\"].shape[0]):\n",
    "          instrument_id=batch[\"instrument\"][sample_index].numpy()\n",
    "          if instrument_id not in instrument_data:\n",
    "            instrument_data[instrument_id]={}\n",
    "            instrument_data[instrument_id][\"z\"]=tf.random.normal([INSTRUMENT_EMBEDDING_SIZE]).numpy()\n",
    "            instrument_data[instrument_id][\"ir\"]=tf.random.normal([IR_SIZE],mean=0,stddev=1e-6).numpy()\n",
    "\n",
    "          batch_z.append(instrument_data[instrument_id][\"z\"])\n",
    "          batch_ir.append(instrument_data[instrument_id][\"ir\"])   \n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "          batch_z_tf = tf.Variable(tf.stack(batch_z))\n",
    "          batch_ir_tf = tf.Variable(tf.stack(batch_ir)) \n",
    "\n",
    "          batch[\"z\"]=tf.tanh(batch_z_tf)\n",
    "          batch[\"ir\"]=tf.tanh(batch_ir_tf)\n",
    "\n",
    "          output=ae(batch,training=False)\n",
    "\n",
    "          loss_value=spectral_loss(batch[\"audio\"],output['audio_synth'])\n",
    "\n",
    "          gradients = tape.gradient(loss_value, [batch_z_tf,batch_ir_tf,*ae.trainable_variables])\n",
    "          gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "\n",
    "          epoch_loss+=loss_value.numpy()\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, [batch_z_tf,batch_ir_tf,*ae.trainable_variables]))\n",
    "\n",
    "        for sample_index in range(batch[\"instrument\"].shape[0]):\n",
    "          # TODO: average instrument embeddings per instrument\n",
    "          instrument_data[instrument_id][\"z\"]=batch_z_tf[sample_index].numpy()\n",
    "          instrument_data[instrument_id][\"ir\"]=batch_ir_tf[sample_index].numpy()\n",
    "        \n",
    "    batch_counter+=1\n",
    "    \n",
    "  plotlosses.update({\n",
    "    'loss': epoch_loss/batch_counter,\n",
    "  })\n",
    "\n",
    "  plotlosses.send()\n",
    "\n",
    "  print(f\"summary nr: {e}\")\n",
    "\n",
    "  print(loss_value)\n",
    "  play(tf.reshape(batch[\"audio\"],(-1)))\n",
    "  play(tf.reshape(output['audio_synth'],(-1)))\n",
    "    \n",
    "  plt.plot(batch[\"ir\"][0])\n",
    "\n",
    "  ae.save_weights(checkpoint_path)\n",
    "\n",
    "  e+=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pydash\n",
    "\n",
    "## validation \n",
    "\n",
    "val_dataset=list(val_dataset)\n",
    "\n",
    "VAL_LR=1e-1\n",
    "\n",
    "# order by velocity\n",
    "val_dataset=pydash.collections.sort_by(val_dataset,lambda x: x[\"velocity\"].numpy())\n",
    "\n",
    "#order by pitch\n",
    "val_dataset=pydash.collections.sort_by(val_dataset,lambda x: x[\"pitch\"].numpy())\n",
    "\n",
    "# group by instrument id\n",
    "val_dataset_by_instrument=pydash.collections.group_by(list(val_dataset),lambda x: str(x[\"instrument\"].numpy())+\" \"+str(x[\"velocity\"].numpy()))\n",
    "\n",
    "fit_iterations=100\n",
    "\n",
    "def rf2cf(row_form):\n",
    "    return {k:[s[k] for s in row_form] for k in row_form[0].keys()}\n",
    "    \n",
    "DEMO_NOTE_SAMPLES=int(0.2*SAMPLE_RATE)\n",
    "\n",
    "for instrument_set in val_dataset_by_instrument.values():    \n",
    "\n",
    "    val_optimizer = tf.keras.optimizers.Adam(learning_rate=VAL_LR)\n",
    "\n",
    "    # fit an embedding to highest and lowest note\n",
    "    \n",
    "    fit_data=[instrument_set[int(len(instrument_set)/2)]]\n",
    "    \n",
    "    N_FIT_SAMPLES=len(fit_data)\n",
    "    \n",
    "    #convert to column form\n",
    "    fit_data = rf2cf(fit_data)\n",
    "        \n",
    "    fit_batch= next(iter(tf.data.Dataset.from_tensor_slices(fit_data).batch(N_FIT_SAMPLES)))\n",
    "    \n",
    "    fit_batch_embeddings=tf.Variable(tf.random.normal([1,INSTRUMENT_EMBEDDING_SIZE]))\n",
    "    \n",
    "    for i in range(fit_iterations):\n",
    "     \n",
    "        with tf.GradientTape() as tape:\n",
    "          fit_batch[\"instrument_embeddings\"]=tf.tile(tf.tanh(fit_batch_embeddings),[N_FIT_SAMPLES,1])\n",
    "\n",
    "          output=ae(fit_batch,training=False)\n",
    "\n",
    "          loss_value=spectral_loss(fit_batch[\"audio\"],output['audio_synth'])\n",
    "\n",
    "          gradients = tape.gradient(loss_value, [fit_batch_embeddings])\n",
    "          gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "        \n",
    "\n",
    "        val_optimizer.apply_gradients(zip(gradients, [fit_batch_embeddings]))\n",
    "        \n",
    "    print(loss_value)\n",
    "           \n",
    "    play(tf.reshape(fit_batch[\"audio\"],(-1)))\n",
    "    play(tf.reshape(output['audio_synth'],(-1)))\n",
    "    \n",
    "    N_SCALE_SAMPLES=len(instrument_set)\n",
    "        \n",
    "    scale_data = rf2cf(instrument_set) \n",
    "    scale_batch= next(iter(tf.data.Dataset.from_tensor_slices(scale_data).batch(N_SCALE_SAMPLES)))\n",
    "    \n",
    "    play(tf.reshape(scale_batch[\"audio\"][:,:DEMO_NOTE_SAMPLES],(-1)))\n",
    "    \n",
    "    scale_batch[\"instrument_embeddings\"] = tf.tile(tf.tanh(fit_batch_embeddings),[N_SCALE_SAMPLES,1])\n",
    "    \n",
    "    print(scale_batch[\"instrument_embeddings\"].shape)\n",
    "    \n",
    "    scale_batch_output=ae(scale_batch,training=False)\n",
    "    \n",
    "    play(tf.reshape(scale_batch_output['audio_synth'][:,:DEMO_NOTE_SAMPLES],(-1)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "slow instrument cloning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
