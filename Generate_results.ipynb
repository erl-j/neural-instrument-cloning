{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and utils\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "import ddsp.training\n",
    "_AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "from IPython.display import Audio, display\n",
    "from livelossplot import PlotLosses\n",
    "import numpy as np\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import time\n",
    "import data\n",
    "import random\n",
    "import copy\n",
    "import pydash\n",
    "import tqdm\n",
    "import soundfile\n",
    "import os\n",
    "import shared_model\n",
    "\n",
    "# define constants\n",
    "CLIP_S=4\n",
    "SAMPLE_RATE=48000\n",
    "N_SAMPLES=SAMPLE_RATE*CLIP_S\n",
    "SEED=1\n",
    "FT_FRAME_RATE=250\n",
    "\n",
    "tf.random.set_seed(\n",
    "    SEED\n",
    ")\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# define some utilis\n",
    "def play(audio):\n",
    "  display(Audio(audio,rate=SAMPLE_RATE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_NSYNTH=False\n",
    "INSTRUMENT_FAMILY=\"Flute\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IR_DURATION=1\n",
    "Z_SIZE=1024 if INSTRUMENT_FAMILY==\"**\" else 512\n",
    "N_INSTRUMENTS=200\n",
    "BIDIRECTIONAL=True\n",
    "USE_F0_CONFIDENCE=True\n",
    "N_NOISE_MAGNITUDES=192\n",
    "N_HARMONICS=192\n",
    "\n",
    "# define loss\n",
    "fft_sizes = [64]\n",
    "while fft_sizes[-1]<SAMPLE_RATE//4:\n",
    "    fft_sizes.append(fft_sizes[-1]*2)\n",
    "\n",
    "spectral_loss = ddsp.losses.SpectralLoss(loss_type='L1',\n",
    "                                            fft_sizes=fft_sizes,\n",
    "                                            mag_weight=1.0,\n",
    "                                            logmag_weight=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_NSYNTH:\n",
    "    tfds.load(\"nsynth/gansynth_subset.f0_and_loudness\",split=\"train\", try_gcs=False,download=True) \n",
    "    trn_data_provider = data.CustomNSynthTfds(data_dir=\"/root/tensorflow_datasets/\",split=\"train\")\n",
    "    tfds.load(\"nsynth/gansynth_subset.f0_and_loudness\",split=\"valid\", try_gcs=False,download=True) \n",
    "    val_data_provider = data.CustomNSynthTfds(data_dir=\"/root/tensorflow_datasets/\",split=\"valid\")\n",
    "    def crepe_is_certain(x):\n",
    "        is_playing = tf.cast(x[\"loudness_db\"]>-100.0,dtype=tf.float32)\n",
    "        average_certainty=tf.reduce_sum(x[\"f0_confidence\"]*is_playing)/tf.reduce_sum(is_playing)\n",
    "        return average_certainty\n",
    "    def preprocess_dataset(dataset):\n",
    "        if INSTRUMENT_FAMILY!=\"all\":\n",
    "            dataset=dataset.filter(lambda x: x[\"instrument_family\"]==INSTRUMENT_FAMILY)\n",
    "        return dataset\n",
    "    trn_dataset = preprocess_dataset(trn_data_provider.get_dataset())\n",
    "    val_dataset = preprocess_dataset(val_data_provider.get_dataset())\n",
    "\n",
    "else:\n",
    "    trn_data_provider=data.MultiTFRecordProvider(f\"datasets/AIR/tfr/dev/{INSTRUMENT_FAMILY}/*\",sample_rate=SAMPLE_RATE)\n",
    "    trn_dataset= trn_data_provider.get_dataset()\n",
    "    \n",
    "    val_data_provider=data.MultiTFRecordProvider(f\"datasets/AIR/tfr/tst/{INSTRUMENT_FAMILY}/*\",sample_rate=SAMPLE_RATE)\n",
    "    val_dataset=val_data_provider.get_dataset(shuffle=False)\n",
    "    \n",
    "# remove some samples if number of recordings greater than model capacity\n",
    "trn_dataset = trn_dataset.filter(lambda x: int(x[\"instrument_idx\"])<N_INSTRUMENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path=f\"checkpoints/48k_{'bidir' if BIDIRECTIONAL else 'unidir'}_z{Z_SIZE}_conv_family_{INSTRUMENT_FAMILY}{'_f0c' if USE_F0_CONFIDENCE else ''}\"\n",
    "training_savedir=f\"./artefacts/training/{INSTRUMENT_FAMILY}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkpoint_path)\n",
    "\n",
    "model=shared_model.get_model(SAMPLE_RATE,CLIP_S,FT_FRAME_RATE,Z_SIZE,N_INSTRUMENTS,IR_DURATION,BIDIRECTIONAL,USE_F0_CONFIDENCE,N_HARMONICS,N_NOISE_MAGNITUDES,losses=[spectral_loss])\n",
    "\n",
    "try:\n",
    "    print(\"loading checkpoint\")\n",
    "    model.load_weights(checkpoint_path)\n",
    "except:\n",
    "    print(\"couldn't load checkpoint\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionnary for results\n",
    "\n",
    "result_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute training loss across dataset\n",
    "\n",
    "if False:\n",
    "\n",
    "    BATCH_SIZE=1\n",
    "    batched_trn_dataset=trn_dataset.shuffle(10000).batch(BATCH_SIZE,drop_remainder=True)\n",
    "\n",
    "    # 1e-4 was good for saxophone (got us to 4.7-ish in 20 hours our so)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "\n",
    "    e=0\n",
    "\n",
    "    batch_counter=0\n",
    "    epoch_loss=0   \n",
    "    for batch in batched_trn_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            model.set_is_shared_trainable(True)\n",
    "            output=model(batch)\n",
    "            loss_value=spectral_loss(batch[\"audio\"],output[\"audio_synth\"])\n",
    "            gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "            epoch_loss+=loss_value.numpy()\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            batch_counter+=1\n",
    "            \n",
    "    result_dict[\"trn_loss\"]=[epoch_loss/batch_counter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def join_batch(batch):\n",
    "    for key in batch.keys():\n",
    "        assert len(batch[key].shape)<3\n",
    "        if len(batch[key].shape)==2:\n",
    "            batch[key]=tf.reshape(batch[key],(1,-1))\n",
    "    return batch\n",
    "\n",
    "def window_signal(a,window_len,hop_len):\n",
    "     assert(a.shape[0]==1)\n",
    "     windows=[]\n",
    "     start_frame=0\n",
    "     while True:\n",
    "        windows.append(a[:,start_frame:start_frame+window_len,...])\n",
    "        start_frame+=hop_len\n",
    "        if start_frame > a.shape[1]-window_len:\n",
    "            break\n",
    "     return tf.concat(windows,axis=0)\n",
    "\n",
    "def window_sample(instance,win_s,hop_s):\n",
    "    instance[\"audio\"]=window_signal(instance[\"audio\"],win_s*SAMPLE_RATE,hop_s*SAMPLE_RATE)\n",
    "    for key in [\"f0_hz\",\"loudness_db\",\"f0_confidence\"]:\n",
    "        instance[key]=window_signal(instance[key],win_s*FT_FRAME_RATE,hop_s*FT_FRAME_RATE)\n",
    "    instance[\"instrument\"]=tf.repeat(instance[\"instrument\"][0],(instance[\"audio\"].shape[0]))\n",
    "    instance[\"instrument_idx\"]=tf.repeat(instance[\"instrument_idx\"][0],(instance[\"audio\"].shape[0]))\n",
    "    #for key,item in instance.items():\n",
    "    #    assert(len(item.shape)<2 or item.shape[0]>1)\n",
    "    return instance\n",
    "\n",
    "def join_and_window(instance,win_s=4,hop_s=1):\n",
    "    return window_sample(join_batch(instance),win_s,hop_s)\n",
    "\n",
    "def rf2cf(row_form):\n",
    "    return {k:[s[k] for s in row_form] for k in row_form[0].keys()}\n",
    "\n",
    "\n",
    "def playback_and_save(x,fn,DEMO_PATH):\n",
    "    print(fn)\n",
    "    play(x)\n",
    "    os.makedirs(DEMO_PATH,exist_ok=True)\n",
    "    path=DEMO_PATH+f\"recording nr: {ii} \"+fn+\".wav\"\n",
    "    soundfile.write(path,x,SAMPLE_RATE)\n",
    "\n",
    "\n",
    "def stitch(audios):\n",
    "\n",
    "  RENDER_OVERLAP_S=1\n",
    "  out=np.zeros(N_SAMPLES*len(audios))\n",
    "  out[:N_SAMPLES]=audios[0]\n",
    "  taper = np.concatenate([np.linspace(0,1,RENDER_OVERLAP_S*SAMPLE_RATE),np.ones(N_SAMPLES-(RENDER_OVERLAP_S*SAMPLE_RATE*2)),np.linspace(1,0,RENDER_OVERLAP_S*SAMPLE_RATE)])\n",
    "  for ai,a in enumerate(audios[1:]):\n",
    "    out[(ai+1)*(N_SAMPLES-RENDER_OVERLAP_S*SAMPLE_RATE):(ai+2)*(N_SAMPLES-RENDER_OVERLAP_S*SAMPLE_RATE)+RENDER_OVERLAP_S*SAMPLE_RATE]+=a*taper\n",
    "\n",
    "  return out\n",
    "\n",
    "\n",
    "def render_example(dataset,model, transform_key=None,transform_fn=lambda x:x):\n",
    "    audio=[]\n",
    "    for batch in dataset:\n",
    "        if transform_key != None:\n",
    "            batch=copy.deepcopy(batch)\n",
    "            batch[transform_key]=transform_fn(batch[transform_key])\n",
    "        output = test_model(batch)\n",
    "        audio.append(output[\"audio_synth\"])\n",
    "    return stitch(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LOAD MODEL FOR FINETUNING\n",
    "\n",
    "def get_finetuning_model(full_ir_duration,free_ir_duration):\n",
    "\n",
    "    # load model\n",
    "    test_model=shared_model.get_model(SAMPLE_RATE,CLIP_S,FT_FRAME_RATE,Z_SIZE,N_INSTRUMENTS,IR_DURATION,BIDIRECTIONAL,USE_F0_CONFIDENCE,N_HARMONICS,N_NOISE_MAGNITUDES,losses=[])\n",
    "    # load model weights       \n",
    "\n",
    "    DEMO_IR_SAMPLES=int(full_ir_duration*SAMPLE_RATE)\n",
    "\n",
    "    test_model.set_is_shared_trainable(True)\n",
    "    test_model.load_weights(checkpoint_path)\n",
    "\n",
    "    test_model.instrument_weight_metadata[\"ir\"][\"initializer\"]=lambda batch_size: tf.zeros([batch_size,int(full_ir_duration*SAMPLE_RATE)])\n",
    "\n",
    "    if free_ir_duration<full_ir_duration:\n",
    "\n",
    "        er_samples=int(free_ir_duration*SAMPLE_RATE)\n",
    "\n",
    "        er_amp=np.ones((er_samples))\n",
    "        er_amp[er_samples//2:er_samples]=np.linspace(1,0,er_samples//2)\n",
    "\n",
    "        frame_rate=\n",
    "        n_filter_bands=100\n",
    "        n_frames=int(frame_rate*DEMO_IR_DURATION)\n",
    "\n",
    "        ir_fn=ddsp.synths.FilteredNoise(n_samples=DEMO_IR_SAMPLES,\n",
    "                                           window_size=750,\n",
    "                                           scale_fn=tf.nn.relu,\n",
    "                                           initial_bias=1e-7)\n",
    "\n",
    "        def processing_fn(batched_feature):\n",
    "\n",
    "            batch_size=batched_feature.shape[0]\n",
    "            er_ir = tf.nn.tanh(batched_feature[:,:er_samples])\n",
    "\n",
    "            er_amp=np.ones(DEMO_IR_SAMPLES)\n",
    "            er_amp[er_samples//2:er_samples]=np.linspace(1,0,er_samples//2)\n",
    "            er_amp[er_samples:]=0\n",
    "\n",
    "            er_amp = er_amp[None,:]\n",
    "            fn_amp= 1-er_amp\n",
    "\n",
    "            fn_mags=tf.reshape(batched_feature[:,er_samples:],[batch_size,n_frames,n_filter_bands])\n",
    "            fn_ir=ir_fn(fn_mags)\n",
    "\n",
    "            ir=fn_ir*fn_amp+tf.pad(er_ir,[[0,0],[0,int(full_ir_duration*SAMPLE_RATE)-er_samples]])*er_amp\n",
    "\n",
    "            #ir = ddsp.core.fft_convolve( fn_ir,er_ir, padding='same', delay_compensation=0)\n",
    "            return ir\n",
    "\n",
    "        test_model.instrument_weight_metadata[\"ir\"][\"processing\"]=processing_fn\n",
    "        test_model.instrument_weight_metadata[\"ir\"][\"initializer\"]=lambda batch_size: tf.zeros([batch_size,er_samples+n_frames*n_filter_bands])\n",
    "        test_model.instrument_weight_metadata[\"wet_gain\"][\"initializer\"]=lambda batch_size: tf.ones([batch_size,1])*0.5\n",
    "\n",
    "    test_model.initialize_instrument_weights()\n",
    "    test_model.set_is_shared_trainable(True)\n",
    "\n",
    "    TMP_CHECKPOINT_PATH=\"artefacts/tmp_checkpoint\"\n",
    "    test_model.save_weights(TMP_CHECKPOINT_PATH)\n",
    "    \n",
    "    test_model.set_is_shared_trainable(True)\n",
    "    test_model.load_weights(TMP_CHECKPOINT_PATH)\n",
    "    test_model.initialize_instrument_weights()\n",
    "    \n",
    "    return test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# FINETUNING HPARAMS\n",
    "TRAIN_SHARED=False\n",
    "N_FIT_SECONDS = 16\n",
    "DEMO_IR_DURATION=1\n",
    "FREE_IR_DURATION=0.2\n",
    "N_FIT_ITERATIONS= 100 if TRAIN_SHARED else int(100*(16/N_FIT_SECONDS))\n",
    "#N_FIT_ITERATIONS=1\n",
    "VAL_LR=3e-5 if TRAIN_SHARED else 2e-3\n",
    "\n",
    "BATCH_SIZE=1\n",
    "\n",
    "# DEMO HPARAMS\n",
    "n_fit_windows=int(N_FIT_SECONDS/CLIP_S)\n",
    "\n",
    "\n",
    "# OUTPUT SETTINGS\n",
    "VERSION=1\n",
    "DEMO_PATH=f\"artefacts/demos/{INSTRUMENT_FAMILY}_{VERSION}_{N_FIT_SECONDS}_{'train_shared' if TRAIN_SHARED else ''}/\"\n",
    "\n",
    "# group by instrument id\n",
    "val_dataset_by_instrument=pydash.collections.group_by(list(val_dataset),lambda x: str(x[\"instrument\"].numpy()))\n",
    "val_dataset_by_instrument = {k:v for k,v in val_dataset_by_instrument.items()}\n",
    "\n",
    "for ii,instrument_set in enumerate(list(val_dataset_by_instrument.values())): \n",
    "    print(f\"instrument nr {ii}\")\n",
    "\n",
    "    # first, we load up a fresh model\n",
    "    test_model=get_finetuning_model(DEMO_IR_DURATION,FREE_IR_DURATION)\n",
    "\n",
    "    #  first, separate the finetuning data from the test data\n",
    "    fit_data_samples=instrument_set[:n_fit_windows]\n",
    "    \n",
    "    # Use second to last 4 windows (16 s) as test data\n",
    "    test_data_samples=instrument_set[len(instrument_set)-5:-1]\n",
    "    \n",
    "    assert (len(instrument_set)-5>=n_fit_windows)\n",
    "\n",
    "    # convert to column form\n",
    "    fit_data = rf2cf(fit_data_samples)\n",
    "\n",
    "    # get one batch for fitting\n",
    "    fit_batch= next(iter(tf.data.Dataset.from_tensor_slices(fit_data).batch(len(list(fit_data)))))\n",
    "    \n",
    "    playback_and_save(tf.reshape(fit_data[\"audio\"],[-1]),\"training data\",DEMO_PATH)\n",
    "\n",
    "    # transform data so that the clips overlap\n",
    "    fit_batch=join_and_window(fit_batch,4,1)\n",
    "    fit_data=tf.data.Dataset.from_tensor_slices(fit_batch)\n",
    "    fit_batched=fit_data.batch(BATCH_SIZE)\n",
    "\n",
    "    # prepare test data\n",
    "    test_data = rf2cf(test_data_samples)\n",
    "    test_batched= tf.data.Dataset.from_tensor_slices(test_data).batch(BATCH_SIZE)\n",
    "\n",
    "    fit_losses=[]\n",
    "    tst_losses=[]\n",
    "\n",
    "    # set up optimizer\n",
    "    val_optimizer = tf.keras.optimizers.Adam(learning_rate=VAL_LR)\n",
    "\n",
    "    for i in tqdm.tqdm(range(N_FIT_ITERATIONS)):\n",
    "        fit_batched_shuffled=fit_batched.shuffle(100)\n",
    "        epoch_loss=0\n",
    "        batch_counter=0\n",
    "        test_epoch_loss=0\n",
    "        test_batch_counter=0\n",
    "\n",
    "        for fit_batch in fit_batched_shuffled:\n",
    "            with tf.GradientTape() as tape:\n",
    "              test_model.set_is_shared_trainable(TRAIN_SHARED)\n",
    "              output = test_model(fit_batch)\n",
    "              loss_value=spectral_loss(fit_batch[\"audio\"],output[\"audio_synth\"])\n",
    "              epoch_loss+=loss_value.numpy()\n",
    "              batch_counter+=1\n",
    "              gradients = tape.gradient(loss_value, test_model.trainable_weights)\n",
    "            val_optimizer.apply_gradients(zip(gradients, test_model.trainable_weights))\n",
    "        fit_losses.append(epoch_loss/batch_counter)\n",
    "\n",
    "        for test_batch in test_batched:\n",
    "            test_model.set_is_shared_trainable(False)\n",
    "            test_output=test_model(test_batch)\n",
    "            loss_value=spectral_loss(test_batch[\"audio\"],test_output[\"audio_synth\"])   \n",
    "            test_epoch_loss+=loss_value.numpy()\n",
    "            test_batch_counter+=1\n",
    "        tst_losses.append(test_epoch_loss/test_batch_counter)\n",
    "\n",
    "        if i%10==0:\n",
    "\n",
    "            print(\"target\")        \n",
    "            play(tf.reshape(fit_batch[\"audio\"],(-1)))\n",
    "\n",
    "            print(\"estimate\")     \n",
    "            play(tf.reshape(output['audio_synth'],(-1)))\n",
    "            # loss plot\n",
    "            plt.plot(tst_losses,label=\"tst\")\n",
    "            plt.plot(fit_losses,label=\"trn\")\n",
    "            plt.yscale(\"log\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            ir=output['ir'][0]\n",
    "\n",
    "            plt.plot(ir)\n",
    "            plt.show()\n",
    "\n",
    "            play(tf.reshape(ir,(-1)))\n",
    "\n",
    "            plt.imshow(ddsp.spectral_ops.compute_mel(ir).numpy().T,aspect=\"auto\",origin=\"lower\")\n",
    "            plt.show()\n",
    "\n",
    "            print(f\"wet gain: {output['wet_gain']['controls']['gain_scaled']}\")\n",
    "            print(f\"dry gain: {output['dry_gain']['controls']['gain_scaled']}\")\n",
    "\n",
    "    plt.plot(tst_losses,label=\"tst\")\n",
    "    plt.plot(fit_losses,label=\"trn\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # RENDER AUDIO EXAMPLES\n",
    "    \n",
    "    # Transform fit data with 3 second skips instead\n",
    "    fit_data = rf2cf(fit_data_samples)\n",
    "\n",
    "    # get one batch for fitting\n",
    "    fit_batch= next(iter(tf.data.Dataset.from_tensor_slices(fit_data).batch(len(list(fit_data)))))\n",
    "    \n",
    "    #playback_and_save(tf.reshape(fit_data[\"audio\"],[-1]),\"training data\",DEMO_PATH)\n",
    "\n",
    "    # transform data so that the clips overlap\n",
    "    fit_batch=join_and_window(fit_batch,4,3)\n",
    "    fit_data=tf.data.Dataset.from_tensor_slices(fit_batch)\n",
    "    fit_batched=fit_data.batch(BATCH_SIZE)\n",
    "\n",
    "    # First fit data\n",
    "    \n",
    "    playback_and_save(render_example(fit_batched,test_model),\"training estimate\",DEMO_PATH)\n",
    "    playback_and_save(render_example(fit_batched,test_model,\"f0_hz\",lambda x:x*(3/4)),\"transposed down a fourth\",DEMO_PATH)\n",
    "    playback_and_save(render_example(fit_batched,test_model,\"f0_hz\",lambda x:x*(4/3)),\"transposed up a fourth\",DEMO_PATH)\n",
    "    playback_and_save(render_example(fit_batched,test_model,\"loudness_db\",lambda x:x-12),\"loudness down 12 db\",DEMO_PATH)\n",
    "    playback_and_save(render_example(fit_batched,test_model,\"loudness_db\",lambda x:x-6),\"loudness down 6 db\",DEMO_PATH)\n",
    "    playback_and_save(render_example(fit_batched,test_model,\"loudness_db\",lambda x:x+6),\"loudness up 6 db\",DEMO_PATH)\n",
    "    playback_and_save(render_example(fit_batched,test_model,\"loudness_db\",lambda x:x+12),\"loudness up 12 db\",DEMO_PATH)\n",
    "    playback_and_save(render_example(fit_batched,test_model,\"f0_confidence\",lambda x:x*0.8),\"pitch confidence * 0.8\",DEMO_PATH)\n",
    "    playback_and_save(render_example(fit_batched,test_model,\"f0_confidence\",lambda x:x*0.5),\"pitch confidence * 0.5\",DEMO_PATH)\n",
    "\n",
    "    # Next test data\n",
    "    \n",
    "    # we need to apply windowing to the signal before rendering\n",
    "    \n",
    "    test_data = rf2cf(test_data_samples)\n",
    "    test_batch= next(iter(tf.data.Dataset.from_tensor_slices(test_data).batch(len(list(test_data)))))\n",
    "    # save test data\n",
    "    playback_and_save(tf.reshape(test_data[\"audio\"],[-1]),\"unseen data\",DEMO_PATH)\n",
    "    # transform data so that the clips overlap\n",
    "    test_batch=join_and_window(test_batch,4,3)\n",
    "    test_data=tf.data.Dataset.from_tensor_slices(test_batch)\n",
    "    test_batched=test_data.batch(BATCH_SIZE)\n",
    "\n",
    "    playback_and_save(render_example(test_batched,test_model),\"unseen estimate\",DEMO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddsp style training\n",
    "strategy =  tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
    "\n",
    "with strategy.scope():\n",
    "    model=shared_model.get_model(SAMPLE_RATE,CLIP_S,FT_FRAME_RATE,Z_SIZE,N_INSTRUMENTS,IR_DURATION,BIDIRECTIONAL,USE_F0_CONFIDENCE,N_HARMONICS,N_NOISE_MAGNITUDES,losses=[spectral_loss])\n",
    "    model.set_is_shared_trainable(True)\n",
    "    trainer=ddsp.training.trainers.Trainer(\n",
    "                   model,\n",
    "                   strategy,\n",
    "                   checkpoints_to_keep=10,\n",
    "                   learning_rate=1e-4,\n",
    "                   lr_decay_steps=10000,\n",
    "                   lr_decay_rate=0.98,\n",
    "                   grad_clip_norm=100000.0)\n",
    "\n",
    "ddsp.training.train_util.train(\n",
    "          trn_data_provider,\n",
    "          trainer,\n",
    "          batch_size=6,\n",
    "          num_steps=1000000,\n",
    "          steps_per_summary=1000,\n",
    "          steps_per_save=1000,\n",
    "          save_dir=training_savedir,\n",
    "          restore_dir=training_savedir,\n",
    "          early_stop_loss_value=None,\n",
    "          report_loss_to_hypertune=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "slow instrument cloning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
