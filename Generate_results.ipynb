{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and utils\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "import ddsp.training\n",
    "_AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "from IPython.display import Audio, display\n",
    "from livelossplot import PlotLosses\n",
    "import numpy as np\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import time\n",
    "import data\n",
    "import random\n",
    "import copy\n",
    "import pydash\n",
    "import tqdm\n",
    "import soundfile\n",
    "import os\n",
    "import shared_model\n",
    "import pandas as pd\n",
    "\n",
    "# define constants\n",
    "CLIP_S=4\n",
    "SAMPLE_RATE=48000\n",
    "N_SAMPLES=SAMPLE_RATE*CLIP_S\n",
    "SEED=1\n",
    "FT_FRAME_RATE=250\n",
    "\n",
    "tf.random.set_seed(\n",
    "    SEED\n",
    ")\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# define some utilis\n",
    "def play(audio):\n",
    "  display(Audio(audio,rate=SAMPLE_RATE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_NSYNTH=False\n",
    "#INSTRUMENT_FAMILY=\"**_WHITHOUT_SAX\"\n",
    "INSTRUMENT_FAMILY=\"Saxophone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IR_DURATION=1\n",
    "Z_SIZE=1024 if INSTRUMENT_FAMILY==\"**\" else 512\n",
    "N_INSTRUMENTS=200\n",
    "BIDIRECTIONAL=True\n",
    "USE_F0_CONFIDENCE=True\n",
    "N_NOISE_MAGNITUDES=192\n",
    "N_HARMONICS=192\n",
    "\n",
    "# define loss\n",
    "fft_sizes = [64]\n",
    "while fft_sizes[-1]<SAMPLE_RATE//4:\n",
    "    fft_sizes.append(fft_sizes[-1]*2)\n",
    "\n",
    "spectral_loss = ddsp.losses.SpectralLoss(loss_type='L1',\n",
    "                                            fft_sizes=fft_sizes,\n",
    "                                            mag_weight=1.0,\n",
    "                                            logmag_weight=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_NSYNTH:\n",
    "    tfds.load(\"nsynth/gansynth_subset.f0_and_loudness\",split=\"train\", try_gcs=False,download=True) \n",
    "    trn_data_provider = data.CustomNSynthTfds(data_dir=\"/root/tensorflow_datasets/\",split=\"train\")\n",
    "    tfds.load(\"nsynth/gansynth_subset.f0_and_loudness\",split=\"valid\", try_gcs=False,download=True) \n",
    "    val_data_provider = data.CustomNSynthTfds(data_dir=\"/root/tensorflow_datasets/\",split=\"valid\")\n",
    "    def crepe_is_certain(x):\n",
    "        is_playing = tf.cast(x[\"loudness_db\"]>-100.0,dtype=tf.float32)\n",
    "        average_certainty=tf.reduce_sum(x[\"f0_confidence\"]*is_playing)/tf.reduce_sum(is_playing)\n",
    "        return average_certainty\n",
    "    def preprocess_dataset(dataset):\n",
    "        if INSTRUMENT_FAMILY!=\"all\":\n",
    "            dataset=dataset.filter(lambda x: x[\"instrument_family\"]==INSTRUMENT_FAMILY)\n",
    "        return dataset\n",
    "    trn_dataset = preprocess_dataset(trn_data_provider.get_dataset())\n",
    "    val_dataset = preprocess_dataset(val_data_provider.get_dataset())\n",
    "\n",
    "else:\n",
    "    \n",
    "    trn_path=f\"datasets/AIR/tfr/dev/{INSTRUMENT_FAMILY}/*\"\n",
    "    val_path=f\"datasets/AIR/tfr/tst/{INSTRUMENT_FAMILY}/*\"\n",
    "    \n",
    "    if INSTRUMENT_FAMILY==\"**_WHITHOUT_SAX\":\n",
    "        print(\"without_sax\")\n",
    "        trn_path=f\"datasets/AIRnoSax/tfr/dev/**/*\"\n",
    "        val_path=f\"datasets/AIRnoSax/tfr/tst/**/*\"\n",
    "    \n",
    "    trn_data_provider=data.MultiTFRecordProvider(trn_path,sample_rate=SAMPLE_RATE)\n",
    "    val_data_provider=data.MultiTFRecordProvider(val_path,sample_rate=SAMPLE_RATE)\n",
    "    trn_dataset= trn_data_provider.get_dataset()\n",
    "    val_dataset=val_data_provider.get_dataset(shuffle=False)\n",
    "    \n",
    "# remove some samples if number of recordings greater than model capacity\n",
    "trn_dataset = trn_dataset.filter(lambda x: int(x[\"instrument_idx\"])<N_INSTRUMENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path=f\"checkpoints/48k_{'bidir' if BIDIRECTIONAL else 'unidir'}_z{Z_SIZE}_conv_family_{INSTRUMENT_FAMILY}{'_f0c' if USE_F0_CONFIDENCE else ''}\"\n",
    "training_savedir=f\"./artefacts/training/{INSTRUMENT_FAMILY}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2')\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'replica_2/multi_instrument_autoencoder/processor_group/harmonic/zeros_like_1' defined at (most recent call last):\n    File \"/usr/lib/python3.8/threading.py\", line 890, in _bootstrap\n      self._bootstrap_inner()\n    File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n      self.run()\n    File \"/usr/local/lib/python3.8/dist-packages/ddsp/training/models/model.py\", line 54, in __call__\n      outputs = super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/workspace/neural-instrument-cloning/shared_model.py\", line 76, in call\n      return super().call(batch,training=training)\n    File \"/usr/local/lib/python3.8/dist-packages/ddsp/training/models/autoencoder.py\", line 62, in call\n      pg_out = self.processor_group(features, return_outputs_dict=True)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/ddsp/processors.py\", line 127, in call\n      controls = self.get_controls(inputs, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/ddsp/processors.py\", line 147, in get_controls\n      return super().call(inputs, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/ddsp/dags.py\", line 132, in call\n      return self.run_dag(inputs, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/gin/config.py\", line 238, in gin_wrapper\n      not_found = object()\n    File \"/usr/local/lib/python3.8/dist-packages/ddsp/dags.py\", line 156, in run_dag\n      for node in self.dag:\n    File \"/usr/local/lib/python3.8/dist-packages/ddsp/dags.py\", line 172, in run_dag\n      if is_processor(module):\n    File \"/usr/local/lib/python3.8/dist-packages/ddsp/dags.py\", line 174, in run_dag\n      module_outputs = module(*inputs, return_outputs_dict=True, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/ddsp/processors.py\", line 65, in call\n      signal = self.get_signal(**controls)\n    File \"/usr/local/lib/python3.8/dist-packages/ddsp/synths.py\", line 142, in get_signal\n      signal = core.harmonic_synthesis(\n    File \"/usr/local/lib/python3.8/dist-packages/ddsp/core.py\", line 927, in harmonic_synthesis\n      audio = oscillator_bank(frequency_envelopes,\n    File \"/usr/local/lib/python3.8/dist-packages/gin/config.py\", line 238, in gin_wrapper\n      not_found = object()\n    File \"/usr/local/lib/python3.8/dist-packages/ddsp/core.py\", line 828, in oscillator_bank\n      amplitude_envelopes = remove_above_nyquist(frequency_envelopes,\n    File \"/usr/local/lib/python3.8/dist-packages/ddsp/core.py\", line 790, in remove_above_nyquist\n      amplitude_envelopes = tf.where(\nNode: 'replica_2/multi_instrument_autoencoder/processor_group/harmonic/zeros_like_1'\n2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED:  OOM when allocating tensor of shape [2,192000,192] and type float\n\t [[{{node replica_2/multi_instrument_autoencoder/processor_group/harmonic/zeros_like_1}}]]\n  (1) CANCELLED:  Function was cancelled before it was started\n0 successful operations.\n0 derived errors ignored. [Op:__inference___call___38576]\n  In call to configurable 'train' (<function train at 0x7fd6c412f040>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m/workspace/neural-instrument-cloning/Generate_results.ipynb Cell 6'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000005vscode-remote?line=5'>6</a>\u001b[0m     model\u001b[39m.\u001b[39mset_is_shared_trainable(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000005vscode-remote?line=6'>7</a>\u001b[0m     trainer\u001b[39m=\u001b[39mddsp\u001b[39m.\u001b[39mtraining\u001b[39m.\u001b[39mtrainers\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000005vscode-remote?line=7'>8</a>\u001b[0m                    model,\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000005vscode-remote?line=8'>9</a>\u001b[0m                    strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000005vscode-remote?line=12'>13</a>\u001b[0m                    lr_decay_rate\u001b[39m=\u001b[39m\u001b[39m0.98\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000005vscode-remote?line=13'>14</a>\u001b[0m                    grad_clip_norm\u001b[39m=\u001b[39m\u001b[39m100000.0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000005vscode-remote?line=15'>16</a>\u001b[0m ddsp\u001b[39m.\u001b[39;49mtraining\u001b[39m.\u001b[39;49mtrain_util\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000005vscode-remote?line=16'>17</a>\u001b[0m           trn_data_provider,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000005vscode-remote?line=17'>18</a>\u001b[0m           trainer,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000005vscode-remote?line=18'>19</a>\u001b[0m           batch_size\u001b[39m=\u001b[39;49m\u001b[39m6\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000005vscode-remote?line=19'>20</a>\u001b[0m           num_steps\u001b[39m=\u001b[39;49m\u001b[39m1000000\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000005vscode-remote?line=20'>21</a>\u001b[0m           steps_per_summary\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000005vscode-remote?line=21'>22</a>\u001b[0m           steps_per_save\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000005vscode-remote?line=22'>23</a>\u001b[0m           save_dir\u001b[39m=\u001b[39;49mtraining_savedir,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000005vscode-remote?line=23'>24</a>\u001b[0m           restore_dir\u001b[39m=\u001b[39;49mtraining_savedir,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000005vscode-remote?line=24'>25</a>\u001b[0m           early_stop_loss_value\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000005vscode-remote?line=25'>26</a>\u001b[0m           report_loss_to_hypertune\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gin/config.py:1605\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/gin/config.py?line=1602'>1603</a>\u001b[0m scope_info \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m in scope \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(scope_str) \u001b[39mif\u001b[39;00m scope_str \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/gin/config.py?line=1603'>1604</a>\u001b[0m err_str \u001b[39m=\u001b[39m err_str\u001b[39m.\u001b[39mformat(name, fn_or_cls, scope_info)\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.8/dist-packages/gin/config.py?line=1604'>1605</a>\u001b[0m utils\u001b[39m.\u001b[39;49maugment_exception_message_and_reraise(e, err_str)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gin/utils.py:41\u001b[0m, in \u001b[0;36maugment_exception_message_and_reraise\u001b[0;34m(exception, message)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/gin/utils.py?line=38'>39</a>\u001b[0m proxy \u001b[39m=\u001b[39m ExceptionProxy()\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/gin/utils.py?line=39'>40</a>\u001b[0m ExceptionProxy\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(exception)\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\n\u001b[0;32m---> <a href='file:///usr/local/lib/python3.8/dist-packages/gin/utils.py?line=40'>41</a>\u001b[0m \u001b[39mraise\u001b[39;00m proxy\u001b[39m.\u001b[39mwith_traceback(exception\u001b[39m.\u001b[39m__traceback__) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gin/config.py:1582\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/gin/config.py?line=1578'>1579</a>\u001b[0m new_kwargs\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/gin/config.py?line=1580'>1581</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.8/dist-packages/gin/config.py?line=1581'>1582</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49mnew_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_kwargs)\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/gin/config.py?line=1582'>1583</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/gin/config.py?line=1583'>1584</a>\u001b[0m   err_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ddsp/training/train_util.py:261\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data_provider, trainer, batch_size, num_steps, steps_per_summary, steps_per_save, save_dir, restore_dir, early_stop_loss_value, report_loss_to_hypertune)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/ddsp/training/train_util.py?line=257'>258</a>\u001b[0m dataset_iter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(dataset)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/ddsp/training/train_util.py?line=259'>260</a>\u001b[0m \u001b[39m# Build model, easiest to just run forward pass.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/dist-packages/ddsp/training/train_util.py?line=260'>261</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mbuild(\u001b[39mnext\u001b[39;49m(dataset_iter))\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/ddsp/training/train_util.py?line=262'>263</a>\u001b[0m \u001b[39m# Load latest checkpoint if one exists in load directory.\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/ddsp/training/train_util.py?line=263'>264</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ddsp/training/trainers.py:143\u001b[0m, in \u001b[0;36mTrainer.build\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/ddsp/training/trainers.py?line=140'>141</a>\u001b[0m \u001b[39m\"\"\"Build the model by running a distributed batch through it.\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/ddsp/training/trainers.py?line=141'>142</a>\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mBuilding the model...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/dist-packages/ddsp/training/trainers.py?line=142'>143</a>\u001b[0m _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(tf\u001b[39m.\u001b[39;49mfunction(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m), batch)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/ddsp/training/trainers.py?line=143'>144</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39msummary()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ddsp/training/trainers.py:138\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/ddsp/training/trainers.py?line=135'>136</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m, fn, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/ddsp/training/trainers.py?line=136'>137</a>\u001b[0m   \u001b[39m\"\"\"Distribute and run function on processors.\"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/dist-packages/ddsp/training/trainers.py?line=137'>138</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrategy\u001b[39m.\u001b[39;49mrun(fn, args\u001b[39m=\u001b[39;49margs, kwargs\u001b[39m=\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:1312\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py?line=1306'>1307</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscope():\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py?line=1307'>1308</a>\u001b[0m   \u001b[39m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py?line=1308'>1309</a>\u001b[0m   \u001b[39m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py?line=1309'>1310</a>\u001b[0m   fn \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py?line=1310'>1311</a>\u001b[0m       fn, autograph_ctx\u001b[39m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py?line=1311'>1312</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extended\u001b[39m.\u001b[39;49mcall_for_each_replica(fn, args\u001b[39m=\u001b[39;49margs, kwargs\u001b[39m=\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:2888\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py?line=2885'>2886</a>\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py?line=2886'>2887</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy()\u001b[39m.\u001b[39mscope():\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py?line=2887'>2888</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_for_each_replica(fn, args, kwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_strategy.py:676\u001b[0m, in \u001b[0;36mMirroredExtended._call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_strategy.py?line=674'>675</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_for_each_replica\u001b[39m(\u001b[39mself\u001b[39m, fn, args, kwargs):\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_strategy.py?line=675'>676</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m mirrored_run\u001b[39m.\u001b[39;49mcall_for_each_replica(\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_strategy.py?line=676'>677</a>\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_container_strategy(), fn, args, kwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py:82\u001b[0m, in \u001b[0;36mcall_for_each_replica\u001b[0;34m(strategy, fn, args, kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py?line=77'>78</a>\u001b[0m     wrapped \u001b[39m=\u001b[39m fn\u001b[39m.\u001b[39m_clone(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py?line=78'>79</a>\u001b[0m         python_function\u001b[39m=\u001b[39mfunctools\u001b[39m.\u001b[39mpartial(call_for_each_replica, strategy,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py?line=79'>80</a>\u001b[0m                                           fn\u001b[39m.\u001b[39mpython_function))\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py?line=80'>81</a>\u001b[0m     _cfer_fn_cache[strategy][fn] \u001b[39m=\u001b[39m wrapped\n\u001b[0;32m---> <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py?line=81'>82</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m wrapped(args, kwargs)\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py?line=83'>84</a>\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py?line=84'>85</a>\u001b[0m   logging\u001b[39m.\u001b[39mlog_first_n(\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py?line=85'>86</a>\u001b[0m       logging\u001b[39m.\u001b[39mWARN, \u001b[39m\"\u001b[39m\u001b[39mUsing \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m eagerly has significant \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py?line=86'>87</a>\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39moverhead currently. We will be working on improving \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py?line=89'>90</a>\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m`run` inside a tf.function to get \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py?line=90'>91</a>\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mthe best performance.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m strategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m5\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py?line=152'>153</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py?line=153'>154</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py?line=154'>155</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py?line=52'>53</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py?line=53'>54</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py?line=54'>55</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py?line=55'>56</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'replica_2/multi_instrument_autoencoder/processor_group/harmonic/zeros_like_1' defined at (most recent call last):\n    File \"/usr/lib/python3.8/threading.py\", line 890, in _bootstrap\n      self._bootstrap_inner()\n    File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n      self.run()\n    File \"/usr/local/lib/python3.8/dist-packages/ddsp/training/models/model.py\", line 54, in __call__\n      outputs = super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/workspace/neural-instrument-cloning/shared_model.py\", line 76, in call\n      return super().call(batch,training=training)\n    File \"/usr/local/lib/python3.8/dist-packages/ddsp/training/models/autoencoder.py\", line 62, in call\n      pg_out = self.processor_group(features, return_outputs_dict=True)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/ddsp/processors.py\", line 127, in call\n      controls = self.get_controls(inputs, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/ddsp/processors.py\", line 147, in get_controls\n      return super().call(inputs, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/ddsp/dags.py\", line 132, in call\n      return self.run_dag(inputs, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/gin/config.py\", line 238, in gin_wrapper\n      not_found = object()\n    File \"/usr/local/lib/python3.8/dist-packages/ddsp/dags.py\", line 156, in run_dag\n      for node in self.dag:\n    File \"/usr/local/lib/python3.8/dist-packages/ddsp/dags.py\", line 172, in run_dag\n      if is_processor(module):\n    File \"/usr/local/lib/python3.8/dist-packages/ddsp/dags.py\", line 174, in run_dag\n      module_outputs = module(*inputs, return_outputs_dict=True, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/ddsp/processors.py\", line 65, in call\n      signal = self.get_signal(**controls)\n    File \"/usr/local/lib/python3.8/dist-packages/ddsp/synths.py\", line 142, in get_signal\n      signal = core.harmonic_synthesis(\n    File \"/usr/local/lib/python3.8/dist-packages/ddsp/core.py\", line 927, in harmonic_synthesis\n      audio = oscillator_bank(frequency_envelopes,\n    File \"/usr/local/lib/python3.8/dist-packages/gin/config.py\", line 238, in gin_wrapper\n      not_found = object()\n    File \"/usr/local/lib/python3.8/dist-packages/ddsp/core.py\", line 828, in oscillator_bank\n      amplitude_envelopes = remove_above_nyquist(frequency_envelopes,\n    File \"/usr/local/lib/python3.8/dist-packages/ddsp/core.py\", line 790, in remove_above_nyquist\n      amplitude_envelopes = tf.where(\nNode: 'replica_2/multi_instrument_autoencoder/processor_group/harmonic/zeros_like_1'\n2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED:  OOM when allocating tensor of shape [2,192000,192] and type float\n\t [[{{node replica_2/multi_instrument_autoencoder/processor_group/harmonic/zeros_like_1}}]]\n  (1) CANCELLED:  Function was cancelled before it was started\n0 successful operations.\n0 derived errors ignored. [Op:__inference___call___38576]\n  In call to configurable 'train' (<function train at 0x7fd6c412f040>)"
     ]
    }
   ],
   "source": [
    "# ddsp style training\n",
    "strategy =  tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
    "\n",
    "with strategy.scope():\n",
    "    model=shared_model.get_model(SAMPLE_RATE,CLIP_S,FT_FRAME_RATE,Z_SIZE,N_INSTRUMENTS,IR_DURATION,BIDIRECTIONAL,USE_F0_CONFIDENCE,N_HARMONICS,N_NOISE_MAGNITUDES,losses=[spectral_loss])\n",
    "    model.set_is_shared_trainable(True)\n",
    "    trainer=ddsp.training.trainers.Trainer(\n",
    "                   model,\n",
    "                   strategy,\n",
    "                   checkpoints_to_keep=10,\n",
    "                   learning_rate=1e-4,\n",
    "                   lr_decay_steps=10000,\n",
    "                   lr_decay_rate=0.98,\n",
    "                   grad_clip_norm=100000.0)\n",
    "\n",
    "ddsp.training.train_util.train(\n",
    "          trn_data_provider,\n",
    "          trainer,\n",
    "          batch_size=6,\n",
    "          num_steps=1000000,\n",
    "          steps_per_summary=1000,\n",
    "          steps_per_save=1000,\n",
    "          save_dir=training_savedir,\n",
    "          restore_dir=training_savedir,\n",
    "          early_stop_loss_value=None,\n",
    "          report_loss_to_hypertune=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkpoint_path)\n",
    "\n",
    "model=shared_model.get_model(SAMPLE_RATE,CLIP_S,FT_FRAME_RATE,Z_SIZE,N_INSTRUMENTS,IR_DURATION,BIDIRECTIONAL,USE_F0_CONFIDENCE,N_HARMONICS,N_NOISE_MAGNITUDES,losses=[spectral_loss])\n",
    "\n",
    "try:\n",
    "    print(\"loading checkpoint\")\n",
    "    model.load_weights(checkpoint_path)\n",
    "except:\n",
    "    print(\"couldn't load checkpoint\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionnary for results\n",
    "\n",
    "result_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute training loss across dataset\n",
    "\n",
    "if False:\n",
    "\n",
    "    BATCH_SIZE=1\n",
    "    batched_trn_dataset=trn_dataset.shuffle(10000).batch(BATCH_SIZE,drop_remainder=True)\n",
    "\n",
    "    # 1e-4 was good for saxophone (got us to 4.7-ish in 20 hours our so)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "\n",
    "    e=0\n",
    "\n",
    "    batch_counter=0\n",
    "    epoch_loss=0   \n",
    "    for batch in batched_trn_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            model.set_is_shared_trainable(True)\n",
    "            output=model(batch)\n",
    "            loss_value=spectral_loss(batch[\"audio\"],output[\"audio_synth\"])\n",
    "            gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "            epoch_loss+=loss_value.numpy()\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            batch_counter+=1\n",
    "            \n",
    "    result_dict[\"trn_loss\"]=[epoch_loss/batch_counter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def join_batch(batch):\n",
    "    for key in batch.keys():\n",
    "        assert len(batch[key].shape)<3\n",
    "        if len(batch[key].shape)==2:\n",
    "            batch[key]=tf.reshape(batch[key],(1,-1))\n",
    "    return batch\n",
    "\n",
    "def window_signal(a,window_len,hop_len):\n",
    "     assert(a.shape[0]==1)\n",
    "     windows=[]\n",
    "     start_frame=0\n",
    "     while True:\n",
    "        windows.append(a[:,start_frame:start_frame+window_len,...])\n",
    "        start_frame+=hop_len\n",
    "        if start_frame > a.shape[1]-window_len:\n",
    "            break\n",
    "     return tf.concat(windows,axis=0)\n",
    "\n",
    "def window_sample(instance,win_s,hop_s):\n",
    "    instance[\"audio\"]=window_signal(instance[\"audio\"],win_s*SAMPLE_RATE,hop_s*SAMPLE_RATE)\n",
    "    for key in [\"f0_hz\",\"loudness_db\",\"f0_confidence\"]:\n",
    "        instance[key]=window_signal(instance[key],win_s*FT_FRAME_RATE,hop_s*FT_FRAME_RATE)\n",
    "    instance[\"instrument\"]=tf.repeat(instance[\"instrument\"][0],(instance[\"audio\"].shape[0]))\n",
    "    instance[\"instrument_idx\"]=tf.repeat(instance[\"instrument_idx\"][0],(instance[\"audio\"].shape[0]))\n",
    "    #for key,item in instance.items():\n",
    "    #    assert(len(item.shape)<2 or item.shape[0]>1)\n",
    "    return instance\n",
    "\n",
    "def join_and_window(instance,win_s=4,hop_s=1):\n",
    "    return window_sample(join_batch(instance),win_s,hop_s)\n",
    "\n",
    "def rf2cf(row_form):\n",
    "    return {k:[s[k] for s in row_form] for k in row_form[0].keys()}\n",
    "\n",
    "\n",
    "def playback_and_save(x,fn,DEMO_PATH):\n",
    "    print(fn)\n",
    "    play(x)\n",
    "    os.makedirs(DEMO_PATH,exist_ok=True)\n",
    "    path=DEMO_PATH+f\"recording nr: {ii} \"+fn+\".wav\"\n",
    "    soundfile.write(path,x,SAMPLE_RATE)\n",
    "\n",
    "\n",
    "def stitch(audios):\n",
    "\n",
    "  RENDER_OVERLAP_S=1\n",
    "  out=np.zeros(N_SAMPLES*len(audios))\n",
    "  tail_taper= np.concatenate([np.ones(RENDER_OVERLAP_S*SAMPLE_RATE),np.ones(N_SAMPLES-(RENDER_OVERLAP_S*SAMPLE_RATE*2)),np.linspace(1,0,RENDER_OVERLAP_S*SAMPLE_RATE)])\n",
    "  head_taper= np.concatenate([np.linspace(0,1,RENDER_OVERLAP_S*SAMPLE_RATE),np.ones(N_SAMPLES-(RENDER_OVERLAP_S*SAMPLE_RATE*2)),np.ones(RENDER_OVERLAP_S*SAMPLE_RATE)])\n",
    "  bi_taper = tail_taper*head_taper\n",
    "\n",
    "  out[:N_SAMPLES]=audios[0]*tail_taper\n",
    "  for ai,a in enumerate(audios[1:-1]):\n",
    "    out[(ai+1)*(N_SAMPLES-RENDER_OVERLAP_S*SAMPLE_RATE):(ai+2)*(N_SAMPLES-RENDER_OVERLAP_S*SAMPLE_RATE)+RENDER_OVERLAP_S*SAMPLE_RATE]+=a*bi_taper\n",
    "    \n",
    "  out[(ai+2)*(N_SAMPLES-RENDER_OVERLAP_S*SAMPLE_RATE):(ai+3)*(N_SAMPLES-RENDER_OVERLAP_S*SAMPLE_RATE)+RENDER_OVERLAP_S*SAMPLE_RATE]+=audios[-1]*head_taper\n",
    "  return out\n",
    "\n",
    "\n",
    "def render_example(dataset,model, transform_key=None,transform_fn=lambda x:x):\n",
    "    audio=[]\n",
    "    for batch in dataset:\n",
    "        if transform_key != None:\n",
    "            batch=copy.deepcopy(batch)\n",
    "            batch[transform_key]=transform_fn(batch[transform_key])\n",
    "        output = test_model(batch)\n",
    "        audio.append(output[\"audio_synth\"])\n",
    "    return stitch(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LOAD MODEL FOR FINETUNING\n",
    "\n",
    "def get_finetuning_model(full_ir_duration,free_ir_duration,checkpoint_path):\n",
    "\n",
    "    # load model\n",
    "    test_model=shared_model.get_model(SAMPLE_RATE,CLIP_S,FT_FRAME_RATE,Z_SIZE,N_INSTRUMENTS,IR_DURATION,BIDIRECTIONAL,USE_F0_CONFIDENCE,N_HARMONICS,N_NOISE_MAGNITUDES,losses=[])\n",
    "    # load model weights       \n",
    "\n",
    "    DEMO_IR_SAMPLES=int(full_ir_duration*SAMPLE_RATE)\n",
    "\n",
    "    test_model.set_is_shared_trainable(True)\n",
    "    if checkpoint_path!=None:\n",
    "        test_model.load_weights(checkpoint_path)\n",
    "\n",
    "    test_model.instrument_weight_metadata[\"ir\"][\"initializer\"]=lambda batch_size: tf.zeros([batch_size,int(full_ir_duration*SAMPLE_RATE)])\n",
    "\n",
    "    if free_ir_duration<full_ir_duration:\n",
    "\n",
    "        er_samples=int(free_ir_duration*SAMPLE_RATE)\n",
    "\n",
    "        er_amp=np.ones((er_samples))\n",
    "        er_amp[er_samples//2:er_samples]=np.linspace(1,0,er_samples//2)\n",
    "\n",
    "        frame_rate=250\n",
    "        n_filter_bands=100\n",
    "        n_frames=int(frame_rate*DEMO_IR_DURATION)\n",
    "\n",
    "        ir_fn=ddsp.synths.FilteredNoise(n_samples=DEMO_IR_SAMPLES,\n",
    "                                           window_size=750,\n",
    "                                           scale_fn=tf.nn.relu,\n",
    "                                           initial_bias=1e-10)\n",
    "\n",
    "        def processing_fn(batched_feature):\n",
    "\n",
    "            batch_size=batched_feature.shape[0]\n",
    "            er_ir = tf.nn.tanh(batched_feature[:,:er_samples])\n",
    "\n",
    "            er_amp=np.ones(DEMO_IR_SAMPLES)\n",
    "            er_amp[er_samples//2:er_samples]=np.linspace(1,0,er_samples//2)\n",
    "            er_amp[er_samples:]=0\n",
    "\n",
    "            er_amp = er_amp[None,:]\n",
    "            fn_amp= 1-er_amp\n",
    "\n",
    "            fn_mags=tf.reshape(batched_feature[:,er_samples:],[batch_size,n_frames,n_filter_bands])\n",
    "            fn_ir=ir_fn(fn_mags)\n",
    "\n",
    "            ir=fn_ir*fn_amp+tf.pad(er_ir,[[0,0],[0,int(full_ir_duration*SAMPLE_RATE)-er_samples]])*er_amp\n",
    "\n",
    "            #ir = ddsp.core.fft_convolve( fn_ir,er_ir, padding='same', delay_compensation=0)\n",
    "            return ir\n",
    "\n",
    "        test_model.instrument_weight_metadata[\"ir\"][\"processing\"]=processing_fn\n",
    "        test_model.instrument_weight_metadata[\"ir\"][\"initializer\"]=lambda batch_size: tf.zeros([batch_size,er_samples+n_frames*n_filter_bands])\n",
    "        test_model.instrument_weight_metadata[\"wet_gain\"][\"initializer\"]=lambda batch_size: tf.ones([batch_size,1])*0.5\n",
    "\n",
    "    test_model.initialize_instrument_weights()\n",
    "    test_model.set_is_shared_trainable(True)\n",
    "\n",
    "    TMP_CHECKPOINT_PATH=\"artefacts/tmp_checkpoint\"\n",
    "    test_model.save_weights(TMP_CHECKPOINT_PATH)\n",
    "    \n",
    "    test_model.set_is_shared_trainable(True)\n",
    "    test_model.load_weights(TMP_CHECKPOINT_PATH)\n",
    "    test_model.initialize_instrument_weights()\n",
    "    \n",
    "    return test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_FIT_ITERATIONS= 100 if TRAIN_SHARED else int(100*(16/N_FIT_SECONDS))\n",
    "VAL_LR=3e-5 if TRAIN_SHARED else 2e-3\n",
    "DEMO_IR_DURATION=1\n",
    "\n",
    "BATCH_SIZE=1\n",
    "\n",
    "# OUTPUT SETTINGS\n",
    "VERSION=2\n",
    "DEMO_PATH=f\"artefacts/demos/{INSTRUMENT_FAMILY}_{VERSION}_{N_FIT_SECONDS}_{'train_shared' if TRAIN_SHARED else ''}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/workspace/neural-instrument-cloning/Generate_results.ipynb Cell 13'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000012vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# comparison test\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000012vscode-remote?line=1'>2</a>\u001b[0m trn_data_provider\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39mMultiTFRecordProvider(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdatasets/comparison_experiment/tfr/dev/*\u001b[39m\u001b[39m\"\u001b[39m,sample_rate\u001b[39m=\u001b[39mSAMPLE_RATE)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000012vscode-remote?line=2'>3</a>\u001b[0m trn_dataset\u001b[39m=\u001b[39m trn_data_provider\u001b[39m.\u001b[39mget_dataset(shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000012vscode-remote?line=4'>5</a>\u001b[0m val_data_provider\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39mMultiTFRecordProvider(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdatasets/comparison_experiment/tfr/tst/*\u001b[39m\u001b[39m\"\u001b[39m,sample_rate\u001b[39m=\u001b[39mSAMPLE_RATE)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# HP TUNING ON OTHER SET?\n",
    "\n",
    "# comparison test\n",
    "trn_data_provider=data.MultiTFRecordProvider(f\"datasets/comparison_experiment/tfr/dev/*\",sample_rate=SAMPLE_RATE)\n",
    "trn_dataset= trn_data_provider.get_dataset(shuffle=False)\n",
    "\n",
    "val_data_provider=data.MultiTFRecordProvider(f\"datasets/comparison_experiment/tfr/tst/*\",sample_rate=SAMPLE_RATE)\n",
    "val_dataset=val_data_provider.get_dataset(shuffle=False)\n",
    "\n",
    "# set adaptation strategy\n",
    "pretrained_checkpoint_path=\"artefacts/training/Saxophone\"\n",
    "finetune_whole=True\n",
    "free_ir_duration=0.2\n",
    "ir_duration=1\n",
    "\n",
    "TRAIN_DATA_DURATIONS = [4*2^i for i in range(6)]\n",
    "\n",
    "# set learning rate and n epochs based on adaptation strategy\n",
    "if pretrained_checkpoint_path!=None:\n",
    "    test_model.set_is_shared_trainable(finetune_whole)\n",
    "    if finetune_whole:\n",
    "        lr=3e-5\n",
    "        n_iterations=1000\n",
    "    if not finetune_whole:\n",
    "        lr=1e-3\n",
    "        n_iterations=1000\n",
    "else:\n",
    "    test_model.set_is_shared_trainable(True)\n",
    "    lr=1e-4\n",
    "    n_iterations=10000\n",
    "\n",
    "\n",
    "for train_data_duration in TRAIN_DATA_DURATIONS:\n",
    "\n",
    "    OUTPUT_PATH=f\"artefacts/comparison_experiment/{pretrained_checkpoint_path}_trn_data_duration={train_data_duration}_finetunewhole={finetune_whole}_free_ir={free_ir_duration}/\"\n",
    "\n",
    "    # load correct amount of training data and window it\n",
    "    # save audio loss, test loss and audio examples\n",
    "\n",
    "    # first, we load up a fresh model\n",
    "    test_model=get_finetuning_model(ir_duration,free_ir_duration,checkpoint_path)\n",
    "\n",
    "    # set up optimizer\n",
    "    val_optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "    iteration_count=0\n",
    "\n",
    "    trn_losses=[]\n",
    "    tst_losses=[]\n",
    "\n",
    "    while iteration_count<n_iterations:\n",
    "\n",
    "        trn_batched_shuffled=trn_batched.shuffle(100000)\n",
    "\n",
    "        epoch_loss=0\n",
    "        batch_counter=0\n",
    "        \n",
    "        test_epoch_loss=0\n",
    "        test_batch_counter=0\n",
    "\n",
    "        for trn_batch in trn_batched_shuffled:\n",
    "            with tf.GradientTape() as tape:\n",
    "                output = test_model(trn_batch)\n",
    "                loss_value=spectral_loss(trn_batch[\"audio\"],output[\"audio_synth\"])\n",
    "                epoch_loss+=loss_value.numpy()\n",
    "                batch_counter+=1\n",
    "                gradients = tape.gradient(loss_value, test_model.trainable_weights)\n",
    "            val_optimizer.apply_gradients(zip(gradients, test_model.trainable_weights))\n",
    "        trn_losses.append(epoch_loss/batch_counter)\n",
    "\n",
    "        for test_batch in test_batched:\n",
    "            test_output=test_model(test_batch)\n",
    "            loss_value=spectral_loss(test_batch[\"audio\"],test_output[\"audio_synth\"])   \n",
    "            test_epoch_loss+=loss_value.numpy()\n",
    "            test_batch_counter+=1\n",
    "        tst_losses.append(test_epoch_loss/test_batch_counter)\n",
    "\n",
    "    # write losses to csv\n",
    "    \n",
    "    pd.DataFrame({\"train_loss\":trn_losses,\"test_loss\":tst_losses}).to_csv(OUTPUT_PATH+\"losses.csv\")\n",
    "\n",
    "    # RENDER AUDIO EXAMPLES\n",
    "        \n",
    "    # Transform trn data with 3 second skips instead\n",
    "    trn_data = rf2cf(trn_data_samples)\n",
    "\n",
    "    # get one batch for trnting\n",
    "    trn_batch= next(iter(tf.data.Dataset.from_tensor_slices(trn_data).batch(len(list(trn_data)))))\n",
    "    \n",
    "    playback_and_save(tf.reshape(trn_data[\"audio\"],[-1]),\"training data\",OUTPUT_PATH)\n",
    "\n",
    "    # transform data so that the clips overlap\n",
    "    trn_batch=join_and_window(trn_batch,4,3)\n",
    "    trn_data=tf.data.Dataset.from_tensor_slices(trn_batch)\n",
    "    trn_batched=trn_data.batch(BATCH_SIZE)\n",
    "\n",
    "    # First trn data\n",
    "    \n",
    "    playback_and_save(render_example(trn_batched,test_model),\"training estimate\",OUTPUT_PATH)\n",
    "    playback_and_save(render_example(trn_batched,test_model,\"f0_hz\",lambda x:x*(3/4)),\"transposed down a fourth\",OUTPUT_PATH)\n",
    "    playback_and_save(render_example(trn_batched,test_model,\"f0_hz\",lambda x:x*(4/3)),\"transposed up a fourth\",OUTPUT_PATH)\n",
    "    playback_and_save(render_example(trn_batched,test_model,\"loudness_db\",lambda x:x-12),\"loudness down 12 db\",OUTPUT_PATH)\n",
    "    playback_and_save(render_example(trn_batched,test_model,\"loudness_db\",lambda x:x-6),\"loudness down 6 db\",OUTPUT_PATH)\n",
    "    playback_and_save(render_example(trn_batched,test_model,\"loudness_db\",lambda x:x+6),\"loudness up 6 db\",OUTPUT_PATH)\n",
    "    playback_and_save(render_example(trn_batched,test_model,\"loudness_db\",lambda x:x+12),\"loudness up 12 db\",OUTPUT_PATH)\n",
    "    playback_and_save(render_example(trn_batched,test_model,\"f0_confidence\",lambda x:x*0.8),\"pitch confidence * 0.8\",OUTPUT_PATH)\n",
    "    playback_and_save(render_example(trn_batched,test_model,\"f0_confidence\",lambda x:x*0.5),\"pitch confidence * 0.5\",OUTPUT_PATH)\n",
    "\n",
    "    # Next test data\n",
    "    \n",
    "    # we need to apply windowing to the signal before rendering\n",
    "    \n",
    "    test_data = rf2cf(test_data_samples)\n",
    "    test_batch= next(iter(tf.data.Dataset.from_tensor_slices(test_data).batch(len(list(test_data)))))\n",
    "    # save test data\n",
    "    playback_and_save(tf.reshape(test_data[\"audio\"],[-1]),\"unseen data\",OUTPUT_PATH)\n",
    "    # transform data so that the clips overlap\n",
    "    test_batch=join_and_window(test_batch,4,3)\n",
    "    test_data=tf.data.Dataset.from_tensor_slices(test_batch)\n",
    "    test_batched=test_data.batch(BATCH_SIZE)\n",
    "\n",
    "    playback_and_save(render_example(test_batched,test_model),\"unseen estimate\",OUTPUT_PATH)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pydash' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/workspace/neural-instrument-cloning/Generate_results.ipynb Cell 13'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000012vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# render demo audio \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000012vscode-remote?line=1'>2</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000012vscode-remote?line=2'>3</a>\u001b[0m \u001b[39m# group by instrument id\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000012vscode-remote?line=3'>4</a>\u001b[0m val_dataset_by_instrument\u001b[39m=\u001b[39mpydash\u001b[39m.\u001b[39mcollections\u001b[39m.\u001b[39mgroup_by(\u001b[39mlist\u001b[39m(val_dataset),\u001b[39mlambda\u001b[39;00m x: \u001b[39mstr\u001b[39m(x[\u001b[39m\"\u001b[39m\u001b[39minstrument\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mnumpy()))\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000012vscode-remote?line=4'>5</a>\u001b[0m val_dataset_by_instrument \u001b[39m=\u001b[39m {k:v \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m val_dataset_by_instrument\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6a6f6e615f6e69635f696d61676535222c2273657474696e6773223a7b22686f7374223a227373683a2f2f64656570737065656368227d7d/workspace/neural-instrument-cloning/Generate_results.ipynb#ch0000012vscode-remote?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m ii,instrument_set \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mlist\u001b[39m(val_dataset_by_instrument\u001b[39m.\u001b[39mvalues())): \n",
      "\u001b[0;31mNameError\u001b[0m: name 'pydash' is not defined"
     ]
    }
   ],
   "source": [
    "# render demo audio \n",
    "\n",
    "# FINETUNING HPARAMS\n",
    "TRAIN_SHARED=False\n",
    "N_FIT_SECONDS = 16\n",
    "FREE_IR_DURATION=0.2\n",
    "n_fit_windows=int(N_FIT_SECONDS/CLIP_S)\n",
    "\n",
    "# group by instrument id\n",
    "val_dataset_by_instrument=pydash.collections.group_by(list(val_dataset),lambda x: str(x[\"instrument\"].numpy()))\n",
    "val_dataset_by_instrument = {k:v for k,v in val_dataset_by_instrument.items()}\n",
    "\n",
    "for ii,instrument_set in enumerate(list(val_dataset_by_instrument.values())): \n",
    "    print(f\"instrument nr {ii}\")\n",
    "\n",
    "    # first, we load up a fresh model\n",
    "    test_model=get_finetuning_model(DEMO_IR_DURATION,FREE_IR_DURATION,checkpoint_path)\n",
    "\n",
    "    #  first, separate the finetuning data from the test data\n",
    "    fit_data_samples=instrument_set[:n_fit_windows]\n",
    "    \n",
    "    # Use second to last 4 windows (16 s) as test data\n",
    "    test_data_samples=instrument_set[len(instrument_set)-5:-1]\n",
    "    \n",
    "    assert (len(instrument_set)-5>=n_fit_windows)\n",
    "\n",
    "    # convert to column form\n",
    "    fit_data = rf2cf(fit_data_samples)\n",
    "\n",
    "    # get one batch for fitting\n",
    "    fit_batch= next(iter(tf.data.Dataset.from_tensor_slices(fit_data).batch(len(list(fit_data)))))\n",
    "    \n",
    "    playback_and_save(tf.reshape(fit_data[\"audio\"],[-1]),\"training data\",DEMO_PATH)\n",
    "\n",
    "    # transform data so that the clips overlap\n",
    "    fit_batch=join_and_window(fit_batch,4,1)\n",
    "    fit_data=tf.data.Dataset.from_tensor_slices(fit_batch)\n",
    "    fit_batched=fit_data.batch(BATCH_SIZE)\n",
    "\n",
    "    # prepare test data\n",
    "    test_data = rf2cf(test_data_samples)\n",
    "    test_batched= tf.data.Dataset.from_tensor_slices(test_data).batch(BATCH_SIZE)\n",
    "\n",
    "    fit_losses=[]\n",
    "    tst_losses=[]\n",
    "\n",
    "    # set up optimizer\n",
    "    val_optimizer = tf.keras.optimizers.Adam(learning_rate=VAL_LR)\n",
    "\n",
    "    for i in tqdm.tqdm(range(N_FIT_ITERATIONS)):\n",
    "        fit_batched_shuffled=fit_batched.shuffle(100)\n",
    "        epoch_loss=0\n",
    "        batch_counter=0\n",
    "        test_epoch_loss=0\n",
    "        test_batch_counter=0\n",
    "\n",
    "        for fit_batch in fit_batched_shuffled:\n",
    "            with tf.GradientTape() as tape:\n",
    "              test_model.set_is_shared_trainable(TRAIN_SHARED)\n",
    "              output = test_model(fit_batch)\n",
    "              loss_value=spectral_loss(fit_batch[\"audio\"],output[\"audio_synth\"])\n",
    "              epoch_loss+=loss_value.numpy()\n",
    "              batch_counter+=1\n",
    "              gradients = tape.gradient(loss_value, test_model.trainable_weights)\n",
    "            val_optimizer.apply_gradients(zip(gradients, test_model.trainable_weights))\n",
    "        fit_losses.append(epoch_loss/batch_counter)\n",
    "\n",
    "        for test_batch in test_batched:\n",
    "            test_model.set_is_shared_trainable(False)\n",
    "            test_output=test_model(test_batch)\n",
    "            loss_value=spectral_loss(test_batch[\"audio\"],test_output[\"audio_synth\"])   \n",
    "            test_epoch_loss+=loss_value.numpy()\n",
    "            test_batch_counter+=1\n",
    "        tst_losses.append(test_epoch_loss/test_batch_counter)\n",
    "\n",
    "        if i%10==0:\n",
    "\n",
    "            print(\"target\")        \n",
    "            play(tf.reshape(fit_batch[\"audio\"],(-1)))\n",
    "\n",
    "            print(\"estimate\")     \n",
    "            play(tf.reshape(output['audio_synth'],(-1)))\n",
    "            # loss plot\n",
    "            plt.plot(tst_losses,label=\"tst\")\n",
    "            plt.plot(fit_losses,label=\"trn\")\n",
    "            plt.yscale(\"log\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            ir=output['ir'][0]\n",
    "\n",
    "            plt.plot(ir)\n",
    "            plt.show()\n",
    "\n",
    "            play(tf.reshape(ir,(-1)))\n",
    "\n",
    "            plt.imshow(ddsp.spectral_ops.compute_mel(ir).numpy().T,aspect=\"auto\",origin=\"lower\")\n",
    "            plt.show()\n",
    "\n",
    "            print(f\"wet gain: {output['wet_gain']['controls']['gain_scaled']}\")\n",
    "            print(f\"dry gain: {output['dry_gain']['controls']['gain_scaled']}\")\n",
    "\n",
    "    plt.plot(tst_losses,label=\"tst\")\n",
    "    plt.plot(fit_losses,label=\"trn\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # RENDER AUDIO EXAMPLES\n",
    "    \n",
    "    # Transform fit data with 3 second skips instead\n",
    "    fit_data = rf2cf(fit_data_samples)\n",
    "\n",
    "    # get one batch for fitting\n",
    "    fit_batch= next(iter(tf.data.Dataset.from_tensor_slices(fit_data).batch(len(list(fit_data)))))\n",
    "    \n",
    "    #playback_and_save(tf.reshape(fit_data[\"audio\"],[-1]),\"training data\",DEMO_PATH)\n",
    "\n",
    "    # transform data so that the clips overlap\n",
    "    fit_batch=join_and_window(fit_batch,4,3)\n",
    "    fit_data=tf.data.Dataset.from_tensor_slices(fit_batch)\n",
    "    fit_batched=fit_data.batch(BATCH_SIZE)\n",
    "\n",
    "    # First fit data\n",
    "    \n",
    "    playback_and_save(render_example(fit_batched,test_model),\"training estimate\",DEMO_PATH)\n",
    "    playback_and_save(render_example(fit_batched,test_model,\"f0_hz\",lambda x:x*(3/4)),\"transposed down a fourth\",DEMO_PATH)\n",
    "    playback_and_save(render_example(fit_batched,test_model,\"f0_hz\",lambda x:x*(4/3)),\"transposed up a fourth\",DEMO_PATH)\n",
    "    playback_and_save(render_example(fit_batched,test_model,\"loudness_db\",lambda x:x-12),\"loudness down 12 db\",DEMO_PATH)\n",
    "    playback_and_save(render_example(fit_batched,test_model,\"loudness_db\",lambda x:x-6),\"loudness down 6 db\",DEMO_PATH)\n",
    "    playback_and_save(render_example(fit_batched,test_model,\"loudness_db\",lambda x:x+6),\"loudness up 6 db\",DEMO_PATH)\n",
    "    playback_and_save(render_example(fit_batched,test_model,\"loudness_db\",lambda x:x+12),\"loudness up 12 db\",DEMO_PATH)\n",
    "    playback_and_save(render_example(fit_batched,test_model,\"f0_confidence\",lambda x:x*0.8),\"pitch confidence * 0.8\",DEMO_PATH)\n",
    "    playback_and_save(render_example(fit_batched,test_model,\"f0_confidence\",lambda x:x*0.5),\"pitch confidence * 0.5\",DEMO_PATH)\n",
    "\n",
    "    # Next test data\n",
    "    \n",
    "    # we need to apply windowing to the signal before rendering\n",
    "    \n",
    "    test_data = rf2cf(test_data_samples)\n",
    "    test_batch= next(iter(tf.data.Dataset.from_tensor_slices(test_data).batch(len(list(test_data)))))\n",
    "    # save test data\n",
    "    playback_and_save(tf.reshape(test_data[\"audio\"],[-1]),\"unseen data\",DEMO_PATH)\n",
    "    # transform data so that the clips overlap\n",
    "    test_batch=join_and_window(test_batch,4,3)\n",
    "    test_data=tf.data.Dataset.from_tensor_slices(test_batch)\n",
    "    test_batched=test_data.batch(BATCH_SIZE)\n",
    "\n",
    "    playback_and_save(render_example(test_batched,test_model),\"unseen estimate\",DEMO_PATH)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "slow instrument cloning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
